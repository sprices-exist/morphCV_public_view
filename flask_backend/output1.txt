=== Folder & File Structure ===
app
init.sql
latex_templates
migrations
user_data
.env
celerybeat-schedule
cv.pdf
docker-compose.yml
Dockerfile
fetch.ps1
gunicorn.conf.py
list_of_cvs.txt
output1.txt
README.md
requirements.txt
run.py
app\api
app\main
app\services
app\tasks
app\utils
app\config.py
app\models.py
app\__init__.py
app\api\auth.py
app\api\cvs.py
app\api\subscription.py
app\api\users.py
app\api\__init__.py
app\main\routes.py
app\main\__init__.py
app\services\auth_service.py
app\services\cv_service.py
app\services\gemini_service.py
app\services\latex_service.py
app\services\payment_service.py
app\tasks\cv_tasks.py
app\utils\decorators.py
app\utils\validators.py
latex_templates\template_1.tex
latex_templates\template_2.tex
latex_templates\template_3.tex
latex_templates\template_4.tex
migrations\versions
migrations\alembic.ini
migrations\env.py
migrations\README
migrations\script.py.mako
migrations\versions\0708ff47c2d5_.py
migrations\versions\1f303c3e9a88_convert_datetime_columns_to_use_.py

=== File Contents (.py and .html) ===
----- FILE: E:\$$latex_project\morphcv\flask_backend\app\api\auth.py -----
from flask import Blueprint, request, jsonify, current_app
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from app.models import db, User, TokenBlacklist
from app.services.auth_service import AuthService
from app.utils.decorators import jwt_required, validate_json
from app.utils.validators import LoginSchema, TokenRefreshSchema
from datetime import datetime, timezone
import logging

# Create blueprint
auth_bp = Blueprint('auth', __name__)

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)

# Initialize services
auth_service = AuthService()

# Validation schemas
login_schema = LoginSchema()
token_refresh_schema = TokenRefreshSchema()

logger = logging.getLogger(__name__)


@auth_bp.route('/google', methods=['POST'])
@limiter.limit("5 per minute")
@validate_json
def google_login():
    """
    Google OAuth login endpoint.
    
    Expected payload:
    {
        "token": "google_oauth_token",
        "user_info": {
            "email": "user@example.com",
            "name": "User Name",
            "picture": "profile_pic_url",
            "sub": "google_user_id"
        }
    }
    """
    try:
        data = request.get_json()
        
        # Validate required fields
        if not data.get('token') or not data.get('user_info'):
            return jsonify({
                'error': 'Missing required fields',
                'message': 'Both token and user_info are required'
            }), 400
        
        user_info = data['user_info']
        required_fields = ['email', 'name', 'sub']
        missing_fields = [field for field in required_fields if not user_info.get(field)]
        
        if missing_fields:
            return jsonify({
                'error': 'Missing user info fields',
                'message': f'Missing fields: {", ".join(missing_fields)}'
            }), 400
        
        # Verify token with Google (simplified for now)
        # In production, you should verify the token with Google's API
        
        # Find or create user
        user = User.query.filter_by(google_id=user_info['sub']).first()
        
        if not user:
            # Check if user exists with same email
            existing_user = User.query.filter_by(email=user_info['email']).first()
            if existing_user:
                # Link Google account to existing user
                existing_user.google_id = user_info['sub']
                existing_user.name = user_info.get('name', existing_user.name)
                existing_user.profile_pic = user_info.get('picture', existing_user.profile_pic)
                user = existing_user
            else:
                # Create new user
                user = User(
                    email=user_info['email'],
                    google_id=user_info['sub'],
                    name=user_info.get('name'),
                    profile_pic=user_info.get('picture')
                )
                db.session.add(user)
        
        # Update last login
        user.last_login = datetime.now(timezone.utc)
        db.session.commit()
        
        # Generate JWT tokens
        access_token, refresh_token = auth_service.generate_tokens(user)
        
        logger.info(f'User {user.email} logged in successfully via Google OAuth')
        
        return jsonify({
            'message': 'Login successful',
            'user': user.to_dict(),
            'access_token': access_token,
            'refresh_token': refresh_token,
            'token_type': 'Bearer'
        }), 200
        
    except Exception as e:
        logger.error(f'Google login error: {str(e)}')
        return jsonify({
            'error': 'Login failed',
            'message': 'An error occurred during login'
        }), 500


@auth_bp.route('/refresh', methods=['POST'])
@limiter.limit("10 per minute")
@validate_json
def refresh_token():
    """
    Refresh JWT access token using refresh token.
    
    Expected payload:
    {
        "refresh_token": "jwt_refresh_token"
    }
    """
    try:
        data = request.get_json()
        refresh_token = data.get('refresh_token')
        
        if not refresh_token:
            return jsonify({
                'error': 'Missing refresh token',
                'message': 'Refresh token is required'
            }), 400
        
        # Validate and process refresh token
        result = auth_service.refresh_access_token(refresh_token)
        
        if result['success']:
            return jsonify({
                'message': 'Token refreshed successfully',
                'access_token': result['access_token'],
                'token_type': 'Bearer'
            }), 200
        else:
            return jsonify({
                'error': 'Token refresh failed',
                'message': result['message']
            }), 401
            
    except Exception as e:
        logger.error(f'Token refresh error: {str(e)}')
        return jsonify({
            'error': 'Token refresh failed',
            'message': 'An error occurred during token refresh'
        }), 500


@auth_bp.route('/logout', methods=['POST'])
@jwt_required
def logout():
    """
    Logout user and blacklist tokens.
    
    Expected payload:
    {
        "refresh_token": "jwt_refresh_token"  # optional
    }
    """
    try:
        current_user = request.current_user
        token_jti = request.token_claims.get('jti')
        
        data = request.get_json() or {}
        refresh_token = data.get('refresh_token')
        
        # Blacklist current access token
        if token_jti:
            blacklist_entry = TokenBlacklist(
                jti=token_jti,
                user_id=current_user.id,
                token_type='access',
                expires_at=datetime.fromtimestamp(request.token_claims.get('exp'), timezone.utc)
            )
            db.session.add(blacklist_entry)
        
        # Blacklist refresh token if provided
        if refresh_token:
            result = auth_service.blacklist_refresh_token(refresh_token, current_user.id)
            if not result['success']:
                logger.warning(f'Failed to blacklist refresh token for user {current_user.id}')
        
        db.session.commit()
        
        logger.info(f'User {current_user.email} logged out successfully')
        
        return jsonify({
            'message': 'Logout successful'
        }), 200
        
    except Exception as e:
        logger.error(f'Logout error: {str(e)}')
        return jsonify({
            'error': 'Logout failed',
            'message': 'An error occurred during logout'
        }), 500


@auth_bp.route('/me', methods=['GET'])
@jwt_required
def get_current_user():
    """Get current user profile information."""
    try:
        current_user = request.current_user
        
        return jsonify({
            'user': current_user.to_dict()
        }), 200
        
    except Exception as e:
        logger.error(f'Get current user error: {str(e)}')
        return jsonify({
            'error': 'Failed to get user info',
            'message': 'An error occurred while fetching user information'
        }), 500


@auth_bp.route('/verify', methods=['POST'])
@jwt_required
def verify_token():
    """Verify if current access token is valid."""
    try:
        current_user = request.current_user
        
        return jsonify({
            'message': 'Token is valid',
            'user_id': current_user.id,
            'email': current_user.email,
            'expires_at': datetime.fromtimestamp(
                request.token_claims.get('exp'), timezone.utc
            ).isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f'Token verification error: {str(e)}')
        return jsonify({
            'error': 'Token verification failed',
            'message': 'An error occurred during token verification'
        }), 500


@auth_bp.route('/revoke-all', methods=['POST'])
@jwt_required
def revoke_all_tokens():
    """Revoke all tokens for the current user (force logout from all devices)."""
    try:
        current_user = request.current_user
        
        # Get all non-expired tokens for the user
        result = auth_service.revoke_all_user_tokens(current_user.id)
        
        if result['success']:
            return jsonify({
                'message': f"Revoked {result['revoked_count']} tokens successfully"
            }), 200
        else:
            return jsonify({
                'error': 'Token revocation failed',
                'message': result['message']
            }), 500
            
    except Exception as e:
        logger.error(f'Revoke all tokens error: {str(e)}')
        return jsonify({
            'error': 'Token revocation failed',
            'message': 'An error occurred during token revocation'
        }), 500


# Error handlers specific to auth routes
@auth_bp.errorhandler(429)
def auth_rate_limit_exceeded(error):
    """Handle rate limit exceeded for auth routes."""
    return jsonify({
        'error': 'Rate Limit Exceeded',
        'message': 'Too many authentication attempts. Please try again later.',
        'retry_after': error.retry_after
    }), 429

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\api\cvs.py -----
from flask import Blueprint, request, jsonify, send_file, current_app
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from app.models import db, CV, CVStatus, DownloadToken
from app.services.cv_service import CVService
from app.tasks.cv_tasks import generate_cv_task, edit_cv_task
from app.utils.decorators import jwt_required, generation_limit_check, validate_json
from app.utils.validators import (
    CVCreateSchema, CVUpdateSchema, CVFilterSchema, 
    format_validation_errors, validate_cv_uuid
)
from marshmallow import ValidationError
from datetime import datetime, timezone, timedelta
import os
import logging

# Create blueprint
cvs_bp = Blueprint('cvs', __name__)

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)

# Initialize services
cv_service = CVService()

# Validation schemas
cv_create_schema = CVCreateSchema()
cv_update_schema = CVUpdateSchema()
cv_filter_schema = CVFilterSchema()

logger = logging.getLogger(__name__)


@cvs_bp.route('', methods=['GET'])
@jwt_required
def list_cvs():
    """
    List user's CVs with pagination and filtering.
    
    Query parameters:
    - page: Page number (default: 1)
    - per_page: Items per page (default: 10)
    - status: Filter by status
    - template_name: Filter by template
    - search: Search in title and user data
    - sort_by: Sort field (default: created_at)
    - sort_order: Sort order (default: desc)
    """
    try:
        current_user = request.current_user
        
        # Validate query parameters
        try:
            query_params = cv_filter_schema.load(request.args)
        except ValidationError as e:
            return jsonify(format_validation_errors(e.messages)), 400
        
        # Get CVs with pagination and filtering
        result = cv_service.list_user_cvs(current_user.id, query_params)
        
        return jsonify({
            'cvs': [cv.to_dict() for cv in result['cvs']],
            'pagination': {
                'page': result['page'],
                'per_page': result['per_page'],
                'total': result['total'],
                'pages': result['pages'],
                'has_next': result['has_next'],
                'has_prev': result['has_prev']
            }
        }), 200
        
    except Exception as e:
        logger.error(f'List CVs error: {str(e)}')
        return jsonify({
            'error': 'Failed to list CVs',
            'message': 'An error occurred while fetching CVs'
        }), 500


@cvs_bp.route('', methods=['POST'])
@jwt_required
@generation_limit_check
@limiter.limit("10 per hour")
@validate_json
def create_cv():
    """
    Create a new CV.
    
    Expected payload:
    {
        "title": "My CV",
        "template_name": "template_1",
        "user_data": {
            "name": "John Doe",
            "email": "john@example.com",
            "experience": "3 years Python development",
            "skills": ["Python", "Flask", "React"],
            "education": "Computer Science Degree"
        },
        "job_description": "Looking for a Python developer..."
    }
    """
    try:
        current_user = request.current_user
        
        # Validate request data
        try:
            cv_data = cv_create_schema.load(request.get_json())
        except ValidationError as e:
            return jsonify(format_validation_errors(e.messages)), 400
        
        # Create CV record
        cv = CV(
            user_id=current_user.id,
            title=cv_data['title'],
            template_name=cv_data['template_name'],
            user_data=str(cv_data['user_data']),  # Store as JSON string
            job_description=cv_data['job_description'],
            status=CVStatus.PENDING
        )
        
        db.session.add(cv)
        db.session.commit()
        
        # Start async CV generation task
        task = generate_cv_task.delay(
            cv.id,
            cv_data['user_data'],
            cv_data['job_description'],
            cv_data['template_name'],
            current_user.user_tier.value
        )
        
        # Update CV with task ID
        cv.task_id = task.id
        cv.status = CVStatus.PROCESSING
        db.session.commit()
        
        # Use generation for free users
        current_user.use_generation()
        
        logger.info(f'Started CV generation for user {current_user.id}, CV {cv.id}')
        
        return jsonify({
            'message': 'CV generation started',
            'cv': cv.to_dict(),
            'task_id': task.id
        }), 201
        
    except Exception as e:
        logger.error(f'Create CV error: {str(e)}')
        return jsonify({
            'error': 'Failed to create CV',
            'message': 'An error occurred while creating CV'
        }), 500


@cvs_bp.route('/<cv_uuid>', methods=['GET'])
@jwt_required
def get_cv(cv_uuid):
    """Get CV details by UUID."""
    try:
        current_user = request.current_user
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to access it'
            }), 404
        
        # Include sensitive data for owner
        cv_data = cv.to_dict(include_sensitive=True)
        
        # Add download URLs if files exist
        if cv.pdf_path:
            cv_data['download_urls'] = {
                'pdf': f'/api/v1/cvs/{cv_uuid}/download?type=pdf'
            }
            if cv.jpg_path:
                cv_data['download_urls']['jpg'] = f'/api/v1/cvs/{cv_uuid}/download?type=jpg'
        
        return jsonify({'cv': cv_data}), 200
        
    except Exception as e:
        logger.error(f'Get CV error: {str(e)}')
        return jsonify({
            'error': 'Failed to get CV',
            'message': 'An error occurred while fetching CV'
        }), 500


@cvs_bp.route('/<cv_uuid>', methods=['PUT'])
@jwt_required
@limiter.limit("20 per hour")
@validate_json
def update_cv(cv_uuid):
    """
    Update/edit CV.
    
    Expected payload:
    {
        "title": "Updated CV Title",
        "user_data": {...},
        "job_description": "Updated job description",
        "edit_instructions": "Make the experience section more detailed"
    }
    """
    try:
        current_user = request.current_user
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Validate request data
        try:
            update_data = cv_update_schema.load(request.get_json())
        except ValidationError as e:
            return jsonify(format_validation_errors(e.messages)), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to edit it'
            }), 404
        
        # Check if CV is in a state that can be edited
        if cv.status in [CVStatus.PROCESSING]:
            return jsonify({
                'error': 'CV is being processed',
                'message': 'Cannot edit CV while it is being processed'
            }), 409
        
        # Update basic fields
        if 'title' in update_data:
            cv.title = update_data['title']
        
        # If this is a regeneration (has edit_instructions or new job_description)
        needs_regeneration = 'edit_instructions' in update_data or 'job_description' in update_data
        
        if needs_regeneration:
            # Update data and start regeneration task
            if 'user_data' in update_data:
                cv.user_data = str(update_data['user_data'])
            if 'job_description' in update_data:
                cv.job_description = update_data['job_description']
            
            # Start editing task
            cv.status = CVStatus.PROCESSING
            
            task = edit_cv_task.delay(
                cv.id,
                update_data.get('edit_instructions', 'Please update the CV based on the new information'),
                current_user.user_tier.value
            )
            
            cv.task_id = task.id
            
            message = 'CV editing started'
            logger.info(f'Started CV editing for user {current_user.id}, CV {cv.id}')
        else:
            # Just update metadata without regeneration
            message = 'CV updated successfully'
            logger.info(f'Updated CV metadata for user {current_user.id}, CV {cv.id}')
        
        cv.updated_at = datetime.now(timezone.utc)
        db.session.commit()
        
        return jsonify({
            'message': message,
            'cv': cv.to_dict(),
            'task_id': cv.task_id if needs_regeneration else None
        }), 200
        
    except Exception as e:
        logger.error(f'Update CV error: {str(e)}')
        return jsonify({
            'error': 'Failed to update CV',
            'message': 'An error occurred while updating CV'
        }), 500


@cvs_bp.route('/<cv_uuid>', methods=['DELETE'])
@jwt_required
def delete_cv(cv_uuid):
    """Delete CV by UUID."""
    try:
        current_user = request.current_user
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to delete it'
            }), 404
        
        # Delete associated files
        cv_service.delete_cv_files(cv)
        
        # Delete CV record
        db.session.delete(cv)
        db.session.commit()
        
        logger.info(f'Deleted CV {cv.id} for user {current_user.id}')
        
        return jsonify({
            'message': 'CV deleted successfully'
        }), 200
        
    except Exception as e:
        logger.error(f'Delete CV error: {str(e)}')
        return jsonify({
            'error': 'Failed to delete CV',
            'message': 'An error occurred while deleting CV'
        }), 500


@cvs_bp.route('/<cv_uuid>/status', methods=['GET'])
@jwt_required
def get_cv_status(cv_uuid):
    """Get CV generation/processing status."""
    try:
        current_user = request.current_user
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to access it'
            }), 404
        
        # Get task status if task is running
        task_status = None
        if cv.task_id: # MODIFIED line: condition is now less restrictive
            task_status = cv_service.get_task_status(cv.task_id)
        
        return jsonify({
            'cv_id': cv.uuid,
            'status': cv.status.value,
            'error_message': cv.error_message,
            'created_at': cv.created_at.isoformat(),
            'updated_at': cv.updated_at.isoformat(),
            'generation_time': cv.generation_time,
            'task_status': task_status,
            'has_files': {
                'pdf': bool(cv.pdf_path),
                'jpg': bool(cv.jpg_path)
            }
        }), 200
        
    except Exception as e:
        logger.error(f'Get CV status error: {str(e)}')
        return jsonify({
            'error': 'Failed to get CV status',
            'message': 'An error occurred while fetching CV status'
        }), 500


@cvs_bp.route('/<cv_uuid>/download', methods=['GET'])
@jwt_required
def download_cv(cv_uuid):
    """
    Download CV file.
    
    Query parameters:
    - type: File type ('pdf' or 'jpg')
    - token: Download token (optional, for temporary access)
    """
    try:
        current_user = request.current_user
        file_type = request.args.get('type', 'pdf')
        download_token = request.args.get('token')
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Validate file type
        if file_type not in ['pdf', 'jpg']:
            return jsonify({
                'error': 'Invalid file type',
                'message': 'File type must be pdf or jpg'
            }), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to access it'
            }), 404
        
        # Check if CV has the requested file
        file_path = cv.pdf_path if file_type == 'pdf' else cv.jpg_path
        
        if not file_path or not os.path.exists(file_path):
            return jsonify({
                'error': 'File not found',
                'message': f'CV {file_type.upper()} file not found'
            }), 404
        
        # Update download timestamp
        cv.last_downloaded = datetime.now(timezone.utc)
        db.session.commit()
        
        # Determine filename
        filename = f"{cv.title.replace(' ', '_')}_{cv.uuid[:8]}.{file_type}"
        
        logger.info(f'User {current_user.id} downloaded CV {cv.id} ({file_type})')
        
        return send_file(
            file_path,
            as_attachment=True,
            download_name=filename,
            mimetype='application/pdf' if file_type == 'pdf' else 'image/jpeg'
        )
        
    except Exception as e:
        logger.error(f'Download CV error: {str(e)}')
        return jsonify({
            'error': 'Failed to download CV',
            'message': 'An error occurred while downloading CV'
        }), 500


@cvs_bp.route('/<cv_uuid>/generate-download-token', methods=['POST'])
@jwt_required
def generate_download_token(cv_uuid):
    """
    Generate temporary download token for sharing.
    
    Expected payload:
    {
        "file_type": "pdf",
        "expires_in": 3600  // seconds, default 1 hour
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json() or {}
        
        file_type = data.get('file_type', 'pdf')
        expires_in = data.get('expires_in', 3600)  # 1 hour default
        
        # Validate UUID format
        try:
            validate_cv_uuid(cv_uuid)
        except ValidationError as e:
            return jsonify({'error': 'Invalid CV UUID'}), 400
        
        # Validate file type
        if file_type not in ['pdf', 'jpg']:
            return jsonify({
                'error': 'Invalid file type',
                'message': 'File type must be pdf or jpg'
            }), 400
        
        # Get CV
        cv = CV.query.filter_by(uuid=cv_uuid, user_id=current_user.id).first()
        
        if not cv:
            return jsonify({
                'error': 'CV not found',
                'message': 'CV not found or you do not have permission to access it'
            }), 404
        
        # Create download token
        download_token = DownloadToken(
            cv_id=cv.id,
            user_id=current_user.id,
            file_type=file_type,
            expires_at=datetime.now(timezone.utc) + timedelta(seconds=expires_in)
        )
        
        db.session.add(download_token)
        db.session.commit()
        
        download_url = f"/api/v1/cvs/{cv_uuid}/download?type={file_type}&token={download_token.token}"
        
        return jsonify({
            'download_token': download_token.token,
            'download_url': download_url,
            'expires_at': download_token.expires_at.isoformat(),
            'file_type': file_type
        }), 201
        
    except Exception as e:
        logger.error(f'Generate download token error: {str(e)}')
        return jsonify({
            'error': 'Failed to generate download token',
            'message': 'An error occurred while generating download token'
        }), 500


# Error handlers specific to CV routes
@cvs_bp.errorhandler(413)
def file_too_large(error):
    """Handle file too large errors."""
    return jsonify({
        'error': 'File Too Large',
        'message': 'The uploaded file is too large. Maximum size is 16MB.',
        'max_size': '16MB'
    }), 413

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\api\subscription.py -----
from flask import Blueprint, request, jsonify, current_app
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from app.models import db, User, UserTier
from app.services.payment_service import PaymentService
from app.utils.decorators import jwt_required, validate_json
from app.utils.validators import SubscriptionCreateSchema, format_validation_errors
from marshmallow import ValidationError
import stripe
import logging

# Create blueprint
subscription_bp = Blueprint('subscription', __name__)

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)

# Initialize services
# payment_service = PaymentService()

# Validation schemas
subscription_create_schema = SubscriptionCreateSchema()

logger = logging.getLogger(__name__)


@subscription_bp.route('', methods=['GET'])
@jwt_required
def get_subscription_status():
    payment_service = PaymentService()

    """Get current user's subscription status."""
    try:
        current_user = request.current_user
        
        # Get subscription details from Stripe if customer exists
        subscription_details = None
        if current_user.stripe_customer_id:
            subscription_details = payment_service.get_customer_subscription(
                current_user.stripe_customer_id
            )
        
        return jsonify({
            'user_tier': current_user.user_tier.value,
            'generations_left': current_user.generations_left,
            'subscription_status': current_user.subscription_status,
            'subscription_current_period_end': current_user.subscription_current_period_end.isoformat() if current_user.subscription_current_period_end else None,
            'stripe_customer_id': current_user.stripe_customer_id,
            'subscription_details': subscription_details
        }), 200
        
    except Exception as e:
        logger.error(f'Get subscription status error: {str(e)}')
        return jsonify({
            'error': 'Failed to get subscription status',
            'message': 'An error occurred while fetching subscription information'
        }), 500


@subscription_bp.route('/checkout', methods=['POST'])
@jwt_required
@limiter.limit("5 per minute")
@validate_json
def create_checkout_session():
    payment_service = PaymentService()

    """
    Create Stripe checkout session for subscription.
    
    Expected payload:
    {
        "price_id": "price_1234567890",
        "success_url": "https://yourapp.com/success",
        "cancel_url": "https://yourapp.com/cancel"
    }
    """
    try:
        current_user = request.current_user
        
        # Validate request data
        try:
            checkout_data = subscription_create_schema.load(request.get_json())
        except ValidationError as e:
            return jsonify(format_validation_errors(e.messages)), 400
        
        # Ensure user has a Stripe customer ID
        if not current_user.stripe_customer_id:
            customer = payment_service.create_customer(
                email=current_user.email,
                name=current_user.name,
                user_id=current_user.id
            )
            
            if not customer:
                return jsonify({
                    'error': 'Failed to create customer',
                    'message': 'Unable to create payment customer'
                }), 500
            
            current_user.stripe_customer_id = customer['id']
            db.session.commit()
        
        # Create checkout session
        session = payment_service.create_checkout_session(
            customer_id=current_user.stripe_customer_id,
            price_id=checkout_data['price_id'],
            success_url=checkout_data.get('success_url', f"{request.host_url}success"),
            cancel_url=checkout_data.get('cancel_url', f"{request.host_url}cancel"),
            user_id=current_user.id
        )
        
        if not session:
            return jsonify({
                'error': 'Failed to create checkout session',
                'message': 'Unable to create payment session'
            }), 500
        
        logger.info(f'Created checkout session for user {current_user.id}')
        
        return jsonify({
            'checkout_session_id': session['id'],
            'checkout_url': session['url'],
            'session_expires_at': session['expires_at']
        }), 201
        
    except Exception as e:
        logger.error(f'Create checkout session error: {str(e)}')
        return jsonify({
            'error': 'Failed to create checkout session',
            'message': 'An error occurred while creating checkout session'
        }), 500


@subscription_bp.route('/portal', methods=['POST'])
@jwt_required
@limiter.limit("5 per minute")
def create_customer_portal():
    payment_service = PaymentService()

    """
    Create Stripe customer portal session for subscription management.
    
    Expected payload:
    {
        "return_url": "https://yourapp.com/dashboard"
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json() or {}
        
        if not current_user.stripe_customer_id:
            return jsonify({
                'error': 'No subscription found',
                'message': 'You need to have an active subscription to access the portal'
            }), 404
        
        # Create customer portal session
        portal_session = payment_service.create_customer_portal_session(
            customer_id=current_user.stripe_customer_id,
            return_url=data.get('return_url', f"{request.host_url}dashboard")
        )
        
        if not portal_session:
            return jsonify({
                'error': 'Failed to create portal session',
                'message': 'Unable to create customer portal session'
            }), 500
        
        logger.info(f'Created customer portal session for user {current_user.id}')
        
        return jsonify({
            'portal_url': portal_session['url']
        }), 201
        
    except Exception as e:
        logger.error(f'Create customer portal error: {str(e)}')
        return jsonify({
            'error': 'Failed to create customer portal',
            'message': 'An error occurred while creating customer portal'
        }), 500


@subscription_bp.route('/webhook', methods=['POST'])
@limiter.limit("100 per minute")
def stripe_webhook():
    payment_service = PaymentService()

    """
    Handle Stripe webhooks for subscription events.
    """
    try:
        payload = request.get_data()
        sig_header = request.headers.get('Stripe-Signature')
        
        if not sig_header:
            logger.warning('Missing Stripe-Signature header')
            return jsonify({'error': 'Missing signature'}), 400
        
        # Verify webhook signature
        try:
            event = stripe.Webhook.construct_event(
                payload, sig_header, current_app.config['STRIPE_WEBHOOK_SECRET']
            )
        except ValueError:
            logger.error('Invalid payload in webhook')
            return jsonify({'error': 'Invalid payload'}), 400
        except stripe.error.SignatureVerificationError:
            logger.error('Invalid signature in webhook')
            return jsonify({'error': 'Invalid signature'}), 400
        
        # Handle the event
        event_type = event['type']
        event_data = event['data']['object']
        
        logger.info(f'Received Stripe webhook: {event_type}')
        
        if event_type == 'customer.subscription.created':
            payment_service.handle_subscription_created(event_data)
        elif event_type == 'customer.subscription.updated':
            payment_service.handle_subscription_updated(event_data)
        elif event_type == 'customer.subscription.deleted':
            payment_service.handle_subscription_cancelled(event_data)
        elif event_type == 'invoice.payment_succeeded':
            payment_service.handle_payment_succeeded(event_data)
        elif event_type == 'invoice.payment_failed':
            payment_service.handle_payment_failed(event_data)
        elif event_type == 'customer.created':
            payment_service.handle_customer_created(event_data)
        elif event_type == 'customer.updated':
            payment_service.handle_customer_updated(event_data)
        else:
            logger.info(f'Unhandled webhook event type: {event_type}')
        
        return jsonify({'status': 'success'}), 200
        
    except Exception as e:
        logger.error(f'Stripe webhook error: {str(e)}')
        return jsonify({
            'error': 'Webhook processing failed',
            'message': 'An error occurred while processing the webhook'
        }), 500


@subscription_bp.route('/prices', methods=['GET'])
def get_subscription_prices():
    payment_service = PaymentService()

    """Get available subscription prices from Stripe."""
    try:
        prices = payment_service.get_subscription_prices()
        
        if prices is None:
            return jsonify({
                'error': 'Failed to fetch prices',
                'message': 'Unable to retrieve subscription prices'
            }), 500
        
        return jsonify({
            'prices': prices
        }), 200
        
    except Exception as e:
        logger.error(f'Get subscription prices error: {str(e)}')
        return jsonify({
            'error': 'Failed to get prices',
            'message': 'An error occurred while fetching subscription prices'
        }), 500


@subscription_bp.route('/cancel', methods=['POST'])
@jwt_required
@limiter.limit("5 per minute")
def cancel_subscription():
    payment_service = PaymentService()

    """
    Cancel current subscription.
    
    Expected payload:
    {
        "cancel_at_period_end": true,
        "reason": "Not satisfied with service"
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json() or {}
        
        if not current_user.subscription_id:
            return jsonify({
                'error': 'No active subscription',
                'message': 'You do not have an active subscription to cancel'
            }), 404
        
        cancel_at_period_end = data.get('cancel_at_period_end', True)
        reason = data.get('reason', 'Customer requested cancellation')
        
        # Cancel subscription
        result = payment_service.cancel_subscription(
            subscription_id=current_user.subscription_id,
            cancel_at_period_end=cancel_at_period_end,
            reason=reason
        )
        
        if not result:
            return jsonify({
                'error': 'Failed to cancel subscription',
                'message': 'Unable to cancel subscription'
            }), 500
        
        logger.info(f'Cancelled subscription for user {current_user.id}')
        
        return jsonify({
            'message': 'Subscription cancelled successfully',
            'cancel_at_period_end': cancel_at_period_end,
            'effective_date': result.get('current_period_end')
        }), 200
        
    except Exception as e:
        logger.error(f'Cancel subscription error: {str(e)}')
        return jsonify({
            'error': 'Failed to cancel subscription',
            'message': 'An error occurred while cancelling subscription'
        }), 500


@subscription_bp.route('/reactivate', methods=['POST'])
@jwt_required
@limiter.limit("5 per minute")
def reactivate_subscription():
    payment_service = PaymentService()

    """Reactivate a cancelled subscription (if still in current period)."""
    try:
        current_user = request.current_user
        
        if not current_user.subscription_id:
            return jsonify({
                'error': 'No subscription found',
                'message': 'You do not have a subscription to reactivate'
            }), 404
        
        # Reactivate subscription
        result = payment_service.reactivate_subscription(current_user.subscription_id)
        
        if not result:
            return jsonify({
                'error': 'Failed to reactivate subscription',
                'message': 'Unable to reactivate subscription'
            }), 500
        
        logger.info(f'Reactivated subscription for user {current_user.id}')
        
        return jsonify({
            'message': 'Subscription reactivated successfully',
            'subscription_status': result.get('status'),
            'current_period_end': result.get('current_period_end')
        }), 200
        
    except Exception as e:
        logger.error(f'Reactivate subscription error: {str(e)}')
        return jsonify({
            'error': 'Failed to reactivate subscription',
            'message': 'An error occurred while reactivating subscription'
        }), 500


@subscription_bp.route('/usage', methods=['GET'])
@jwt_required
def get_usage_statistics():
    payment_service = PaymentService()

    """Get current billing period usage statistics."""
    try:
        current_user = request.current_user
        
        # Get CV statistics for current user
        from app.services.cv_service import CVService
        cv_service = CVService()
        cv_stats = cv_service.get_user_cv_statistics(current_user.id)
        
        # Calculate usage based on subscription tier
        usage_stats = {
            'user_tier': current_user.user_tier.value,
            'generations_used': cv_stats['total_cvs'],
            'generations_left': current_user.generations_left,
            'successful_generations': cv_stats['successful_cvs'],
            'failed_generations': cv_stats['failed_cvs'],
            'success_rate': cv_stats['success_rate'],
            'most_used_template': cv_stats['most_used_template'],
            'average_generation_time': cv_stats['average_generation_time']
        }
        
        # Add tier-specific limits
        if current_user.user_tier == UserTier.FREE:
            usage_stats.update({
                'generation_limit': 2,
                'unlimited_generations': False,
                'can_edit_cvs': False,
                'has_priority_support': False
            })
        elif current_user.user_tier == UserTier.PRO:
            usage_stats.update({
                'generation_limit': None,
                'unlimited_generations': True,
                'can_edit_cvs': True,
                'has_priority_support': False
            })
        elif current_user.user_tier == UserTier.ENTERPRISE:
            usage_stats.update({
                'generation_limit': None,
                'unlimited_generations': True,
                'can_edit_cvs': True,
                'has_priority_support': True,
                'has_batch_generation': True
            })
        
        return jsonify(usage_stats), 200
        
    except Exception as e:
        logger.error(f'Get usage statistics error: {str(e)}')
        return jsonify({
            'error': 'Failed to get usage statistics',
            'message': 'An error occurred while fetching usage information'
        }), 500


# Error handlers specific to subscription routes
@subscription_bp.errorhandler(429)
def subscription_rate_limit_exceeded(error):
    payment_service = PaymentService()

    """Handle rate limit exceeded for subscription routes."""
    return jsonify({
        'error': 'Rate Limit Exceeded',
        'message': 'Too many subscription requests. Please try again later.',
        'retry_after': error.retry_after
    }), 429

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\api\users.py -----
from flask import Blueprint, request, jsonify
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from app.models import db, User
from app.services.cv_service import CVService
from app.utils.decorators import jwt_required, validate_json
from app.utils.validators import UserUpdateSchema, format_validation_errors
from marshmallow import ValidationError
from datetime import datetime, timezone
import logging

# Create blueprint
users_bp = Blueprint('users', __name__)

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)

# Initialize services
cv_service = CVService()

# Validation schemas
user_update_schema = UserUpdateSchema()

logger = logging.getLogger(__name__)


@users_bp.route('/profile', methods=['GET'])
@jwt_required
def get_user_profile():
    """Get current user's complete profile information."""
    try:
        current_user = request.current_user
        
        # Get CV statistics
        cv_stats = cv_service.get_user_cv_statistics(current_user.id)
        
        profile_data = current_user.to_dict()
        profile_data['cv_statistics'] = cv_stats
        
        return jsonify({
            'user': profile_data
        }), 200
        
    except Exception as e:
        logger.error(f'Get user profile error: {str(e)}')
        return jsonify({
            'error': 'Failed to get profile',
            'message': 'An error occurred while fetching user profile'
        }), 500


@users_bp.route('/profile', methods=['PUT'])
@jwt_required
@limiter.limit("10 per hour")
@validate_json
def update_user_profile():
    """
    Update user profile information.
    
    Expected payload:
    {
        "name": "Updated Name",
        "email": "newemail@example.com"
    }
    """
    try:
        current_user = request.current_user
        
        # Validate request data
        try:
            update_data = user_update_schema.load(request.get_json())
        except ValidationError as e:
            return jsonify(format_validation_errors(e.messages)), 400
        
        # Check if email is being changed and if it's already taken
        if 'email' in update_data and update_data['email'] != current_user.email:
            existing_user = User.query.filter_by(email=update_data['email']).first()
            if existing_user:
                return jsonify({
                    'error': 'Email already taken',
                    'message': 'This email address is already registered'
                }), 409
        
        # Update user fields
        if 'name' in update_data:
            current_user.name = update_data['name']
        
        if 'email' in update_data:
            current_user.email = update_data['email']
        
        current_user.updated_at = datetime.now(timezone.utc)
        db.session.commit()
        
        logger.info(f'Updated profile for user {current_user.id}')
        
        return jsonify({
            'message': 'Profile updated successfully',
            'user': current_user.to_dict()
        }), 200
        
    except Exception as e:
        logger.error(f'Update user profile error: {str(e)}')
        db.session.rollback()
        return jsonify({
            'error': 'Failed to update profile',
            'message': 'An error occurred while updating profile'
        }), 500


@users_bp.route('/statistics', methods=['GET'])
@jwt_required
def get_user_statistics():
    """Get detailed user statistics and analytics."""
    try:
        current_user = request.current_user
        
        # Get CV statistics
        cv_stats = cv_service.get_user_cv_statistics(current_user.id)
        
        # Get recent CVs
        recent_cvs_query = cv_service.list_user_cvs(
            current_user.id, 
            {'page': 1, 'per_page': 5, 'sort_by': 'created_at', 'sort_order': 'desc'}
        )
        
        # Calculate account age
        account_age_days = (datetime.now(timezone.utc) - current_user.created_at).days
        
        statistics = {
            'account_info': {
                'user_tier': current_user.user_tier.value,
                'account_age_days': account_age_days,
                'created_at': current_user.created_at.isoformat(),
                'last_login': current_user.last_login.isoformat() if current_user.last_login else None,
                'generations_left': current_user.generations_left
            },
            'cv_statistics': cv_stats,
            'recent_cvs': [cv.to_dict() for cv in recent_cvs_query['cvs']],
            'subscription_info': {
                'status': current_user.subscription_status,
                'current_period_end': current_user.subscription_current_period_end.isoformat() if current_user.subscription_current_period_end else None,
                'has_active_subscription': bool(current_user.subscription_id and current_user.subscription_status == 'active')
            }
        }
        
        return jsonify(statistics), 200
        
    except Exception as e:
        logger.error(f'Get user statistics error: {str(e)}')
        return jsonify({
            'error': 'Failed to get statistics',
            'message': 'An error occurred while fetching user statistics'
        }), 500


@users_bp.route('/preferences', methods=['GET'])
@jwt_required
def get_user_preferences():
    """Get user preferences and settings."""
    try:
        current_user = request.current_user
        
        # For now, return default preferences
        # In the future, you could add a UserPreferences model
        preferences = {
            'default_template': 'template_1',
            'email_notifications': True,
            'auto_save_drafts': True,
            'preferred_language': 'en',
            'timezone': 'UTC'
        }
        
        return jsonify({
            'preferences': preferences
        }), 200
        
    except Exception as e:
        logger.error(f'Get user preferences error: {str(e)}')
        return jsonify({
            'error': 'Failed to get preferences',
            'message': 'An error occurred while fetching user preferences'
        }), 500


@users_bp.route('/preferences', methods=['PUT'])
@jwt_required
@limiter.limit("20 per hour")
@validate_json
def update_user_preferences():
    """
    Update user preferences.
    
    Expected payload:
    {
        "default_template": "template_2",
        "email_notifications": false,
        "auto_save_drafts": true,
        "preferred_language": "en",
        "timezone": "UTC"
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json()
        
        # For now, just validate the data structure
        # In the future, you could store this in a UserPreferences model
        
        valid_templates = ['template_1', 'template_2', 'template_3', 'template_4']
        if 'default_template' in data and data['default_template'] not in valid_templates:
            return jsonify({
                'error': 'Invalid template',
                'message': f'Template must be one of: {", ".join(valid_templates)}'
            }), 400
        
        # Log preference update for now
        logger.info(f'Updated preferences for user {current_user.id}: {data}')
        
        return jsonify({
            'message': 'Preferences updated successfully',
            'preferences': data
        }), 200
        
    except Exception as e:
        logger.error(f'Update user preferences error: {str(e)}')
        return jsonify({
            'error': 'Failed to update preferences',
            'message': 'An error occurred while updating preferences'
        }), 500


@users_bp.route('/activity', methods=['GET'])
@jwt_required
def get_user_activity():
    """Get user activity log and recent actions."""
    try:
        current_user = request.current_user
        
        # Get recent CVs with more details
        recent_activity_query = cv_service.list_user_cvs(
            current_user.id,
            {'page': 1, 'per_page': 10, 'sort_by': 'updated_at', 'sort_order': 'desc'}
        )
        
        # Format activity data
        activities = []
        for cv in recent_activity_query['cvs']:
            cv_data = cv.to_dict()
            activities.append({
                'id': cv_data['id'],
                'type': 'cv_generation' if cv_data['status'] == 'success' else 'cv_attempt',
                'title': cv_data['title'],
                'status': cv_data['status'],
                'template_used': cv_data['template_name'],
                'created_at': cv_data['created_at'],
                'updated_at': cv_data['updated_at'],
                'generation_time': cv_data['generation_time']
            })
        
        # Add login activity
        if current_user.last_login:
            activities.append({
                'id': f'login_{current_user.id}',
                'type': 'login',
                'title': 'User Login',
                'status': 'success',
                'created_at': current_user.last_login.isoformat(),
                'updated_at': current_user.last_login.isoformat()
            })
        
        # Sort by updated_at
        activities.sort(key=lambda x: x['updated_at'], reverse=True)
        
        activity_summary = {
            'total_activities': len(activities),
            'recent_activities': activities[:10],
            'activity_types': {
                'cv_generations': len([a for a in activities if a['type'] == 'cv_generation']),
                'cv_attempts': len([a for a in activities if a['type'] == 'cv_attempt']),
                'logins': len([a for a in activities if a['type'] == 'login'])
            }
        }
        
        return jsonify(activity_summary), 200
        
    except Exception as e:
        logger.error(f'Get user activity error: {str(e)}')
        return jsonify({
            'error': 'Failed to get activity',
            'message': 'An error occurred while fetching user activity'
        }), 500


@users_bp.route('/export', methods=['POST'])
@jwt_required
@limiter.limit("5 per hour")
def export_user_data():
    """
    Export user data (GDPR compliance).
    
    Expected payload:
    {
        "format": "json",  // or "csv"
        "include_cvs": true,
        "include_activity": true
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json() or {}
        
        export_format = data.get('format', 'json')
        include_cvs = data.get('include_cvs', True)
        include_activity = data.get('include_activity', True)
        
        if export_format not in ['json', 'csv']:
            return jsonify({
                'error': 'Invalid format',
                'message': 'Format must be json or csv'
            }), 400
        
        # Prepare export data
        export_data = {
            'user_profile': current_user.to_dict(),
            'export_timestamp': datetime.now(timezone.utc).isoformat(),
            'export_format': export_format
        }
        
        if include_cvs:
            all_cvs_query = cv_service.list_user_cvs(
                current_user.id,
                {'page': 1, 'per_page': 1000, 'sort_by': 'created_at', 'sort_order': 'desc'}
            )
            export_data['cvs'] = [cv.to_dict(include_sensitive=True) for cv in all_cvs_query['cvs']]
        
        if include_activity:
            cv_stats = cv_service.get_user_cv_statistics(current_user.id)
            export_data['statistics'] = cv_stats
        
        logger.info(f'Exported data for user {current_user.id} in {export_format} format')
        
        return jsonify({
            'message': 'Data exported successfully',
            'export_data': export_data,
            'data_size': len(str(export_data)),
            'includes': {
                'profile': True,
                'cvs': include_cvs,
                'activity': include_activity
            }
        }), 200
        
    except Exception as e:
        logger.error(f'Export user data error: {str(e)}')
        return jsonify({
            'error': 'Failed to export data',
            'message': 'An error occurred while exporting user data'
        }), 500


@users_bp.route('/delete-account', methods=['DELETE'])
@jwt_required
@limiter.limit("1 per day")
def delete_user_account():
    """
    Delete user account and all associated data.
    This is irreversible and for GDPR compliance.
    
    Expected payload:
    {
        "confirmation": "DELETE_MY_ACCOUNT",
        "reason": "Optional reason for deletion"
    }
    """
    try:
        current_user = request.current_user
        data = request.get_json() or {}
        
        # Require explicit confirmation
        if data.get('confirmation') != 'DELETE_MY_ACCOUNT':
            return jsonify({
                'error': 'Invalid confirmation',
                'message': 'Please provide the exact confirmation text: DELETE_MY_ACCOUNT'
            }), 400
        
        user_id = current_user.id
        user_email = current_user.email
        
        # Delete all user CVs and files
        all_cvs_query = cv_service.list_user_cvs(
            current_user.id,
            {'page': 1, 'per_page': 1000}
        )
        
        deleted_cvs = 0
        for cv in all_cvs_query['cvs']:
            cv_service.delete_cv_files(cv)
            db.session.delete(cv)
            deleted_cvs += 1
        
        # Delete user account
        db.session.delete(current_user)
        db.session.commit()
        
        logger.warning(f'Deleted user account {user_id} ({user_email}) and {deleted_cvs} CVs. Reason: {data.get("reason", "Not provided")}')
        
        return jsonify({
            'message': 'Account deleted successfully',
            'deleted_items': {
                'user_account': 1,
                'cvs': deleted_cvs
            }
        }), 200
        
    except Exception as e:
        logger.error(f'Delete user account error: {str(e)}')
        db.session.rollback()
        return jsonify({
            'error': 'Failed to delete account',
            'message': 'An error occurred while deleting account'
        }), 500


# Error handlers specific to user routes
@users_bp.errorhandler(429)
def users_rate_limit_exceeded(error):
    """Handle rate limit exceeded for user routes."""
    return jsonify({
        'error': 'Rate Limit Exceeded',
        'message': 'Too many requests. Please try again later.',
        'retry_after': error.retry_after
    }), 429

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\api\__init__.py -----
from flask import Blueprint

# Create main API blueprint
api_bp = Blueprint('api', __name__)

# Import all API routes to register them
from app.api import auth, cvs, subscription, users

# Register sub-blueprints
api_bp.register_blueprint(auth.auth_bp, url_prefix='/auth')
api_bp.register_blueprint(cvs.cvs_bp, url_prefix='/cvs')
api_bp.register_blueprint(subscription.subscription_bp, url_prefix='/subscription')
api_bp.register_blueprint(users.users_bp, url_prefix='/users')

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\main\routes.py -----
from flask import render_template, send_from_directory, current_app, jsonify
from app.main import main_bp
import os


@main_bp.route('/')
def index():
    """Serve React frontend index.html in production."""
    try:
        # In production, serve the React build
        static_folder = current_app.static_folder
        if static_folder and os.path.exists(os.path.join(static_folder, 'index.html')):
            return send_from_directory(static_folder, 'index.html')
        else:
            # Development fallback - API status
            return jsonify({
                'service': 'MorphCV API',
                'status': 'running',
                'version': '1.0.0',
                'endpoints': {
                    'auth': '/api/v1/auth',
                    'cvs': '/api/v1/cvs',
                    'subscription': '/api/v1/subscription',
                    'users': '/api/v1/users'
                },
                'documentation': '/api/v1/docs'
            })
    except Exception as e:
        return jsonify({
            'error': 'Service unavailable',
            'message': str(e)
        }), 500


@main_bp.route('/<path:path>')
def serve_static(path):
    """Serve React static files."""
    try:
        static_folder = current_app.static_folder
        if static_folder and os.path.exists(os.path.join(static_folder, path)):
            return send_from_directory(static_folder, path)
        else:
            # If file doesn't exist, serve index.html for client-side routing
            return send_from_directory(static_folder, 'index.html')
    except Exception:
        return jsonify({
            'error': 'File not found',
            'message': 'The requested resource could not be found'
        }), 404


@main_bp.route('/api/v1/docs')
def api_documentation():
    """Serve API documentation."""
    return jsonify({
        'title': 'MorphCV API Documentation',
        'version': '1.0.0',
        'description': 'AI-powered CV generation and management API',
        'base_url': '/api/v1',
        'authentication': {
            'type': 'Bearer Token (JWT)',
            'header': 'Authorization: Bearer <token>',
            'login_endpoint': '/api/v1/auth/google'
        },
        'endpoints': {
            'authentication': {
                'POST /auth/google': 'Google OAuth login',
                'POST /auth/refresh': 'Refresh access token',
                'POST /auth/logout': 'Logout user',
                'GET /auth/me': 'Get current user profile'
            },
            'cv_management': {
                'GET /cvs': 'List user CVs with pagination',
                'POST /cvs': 'Generate new CV',
                'GET /cvs/{uuid}': 'Get CV details',
                'PUT /cvs/{uuid}': 'Edit CV',
                'DELETE /cvs/{uuid}': 'Delete CV',
                'GET /cvs/{uuid}/status': 'Get generation status',
                'GET /cvs/{uuid}/download': 'Download CV file'
            },
            'subscription': {
                'GET /subscription': 'Get subscription status',
                'POST /subscription/checkout': 'Create payment session',
                'POST /subscription/portal': 'Customer portal',
                'GET /subscription/prices': 'Available prices'
            },
            'user_management': {
                'GET /users/profile': 'Get user profile',
                'PUT /users/profile': 'Update profile',
                'GET /users/statistics': 'User statistics',
                'POST /users/export': 'Export user data'
            }
        },
        'rate_limits': {
            'authentication': '5 requests per minute',
            'cv_generation': '10 requests per hour',
            'api_general': '100 requests per hour'
        },
        'response_format': {
            'success': {'data': '...', 'message': 'Success'},
            'error': {'error': 'Error Type', 'message': 'Description', 'status_code': 400}
        }
    })


@main_bp.route('/favicon.ico')
def favicon():
    """Serve favicon."""
    try:
        static_folder = current_app.static_folder
        if static_folder:
            return send_from_directory(static_folder, 'favicon.ico')
    except Exception:
        pass
    
    # Return empty response if favicon not found
    return '', 204

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\main\__init__.py -----
from flask import Blueprint

main_bp = Blueprint('main', __name__)

from app.main import routes

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\services\auth_service.py -----
import jwt
import uuid
from datetime import datetime, timezone, timedelta
from flask import current_app
from app.models import db, User, TokenBlacklist
import logging

logger = logging.getLogger(__name__)


class AuthService:
    """Service for handling authentication, JWT tokens, and user management."""
    
    def generate_tokens(self, user):
        """
        Generate access and refresh JWT tokens for a user.
        
        Args:
            user (User): User object
            
        Returns:
            tuple: (access_token, refresh_token)
        """
        now = datetime.now(timezone.utc)
        
        # Generate unique JTIs
        access_jti = str(uuid.uuid4())
        refresh_jti = str(uuid.uuid4())
        
        # Access token payload
        access_payload = {
            'user_id': user.id,
            'email': user.email,
            'user_tier': user.user_tier.value,
            'jti': access_jti,
            'type': 'access',
            'iat': now,
            'exp': now + current_app.config['JWT_ACCESS_TOKEN_EXPIRES']
        }
        
        # Refresh token payload
        refresh_payload = {
            'user_id': user.id,
            'jti': refresh_jti,
            'type': 'refresh',
            'iat': now,
            'exp': now + current_app.config['JWT_REFRESH_TOKEN_EXPIRES']
        }
        
        # Encode tokens
        access_token = jwt.encode(
            access_payload,
            current_app.config['JWT_SECRET_KEY'],
            algorithm=current_app.config['JWT_ALGORITHM']
        )
        
        refresh_token = jwt.encode(
            refresh_payload,
            current_app.config['JWT_SECRET_KEY'],
            algorithm=current_app.config['JWT_ALGORITHM']
        )
        
        logger.info(f'Generated tokens for user {user.id}')
        
        return access_token, refresh_token
    
    def decode_token(self, token):
        """
        Decode and validate JWT token.
        
        Args:
            token (str): JWT token string
            
        Returns:
            dict: Token payload if valid, None if invalid
        """
        try:
            payload = jwt.decode(
                token,
                current_app.config['JWT_SECRET_KEY'],
                algorithms=[current_app.config['JWT_ALGORITHM']]
            )
            
            # Check if token is blacklisted
            if self.is_token_blacklisted(payload.get('jti')):
                logger.warning("Attempted use of blacklisted token")
                return None
            
            return payload
            
        except jwt.ExpiredSignatureError:
            logger.info('Token has expired')
            return None
        except jwt.InvalidTokenError as e:
            logger.warning(f'Invalid token: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Token decode error: {str(e)}')
            return None
    
    def refresh_access_token(self, refresh_token):
        """
        Generate new access token using refresh token.
        
        Args:
            refresh_token (str): Refresh token string
            
        Returns:
            dict: Result with success status and new access token or error message
        """
        try:
            # Decode refresh token
            payload = self.decode_token(refresh_token)
            
            if not payload:
                return {'success': False, 'message': 'Invalid or expired refresh token'}
            
            if payload.get('type') != 'refresh':
                return {'success': False, 'message': 'Invalid token type'}
            
            # Get user
            user = User.query.get(payload.get('user_id'))
            if not user:
                return {'success': False, 'message': 'User not found'}
            
            # Generate new access token
            now = datetime.now(timezone.utc)
            access_jti = str(uuid.uuid4())
            
            access_payload = {
                'user_id': user.id,
                'email': user.email,
                'user_tier': user.user_tier.value,
                'jti': access_jti,
                'type': 'access',
                'iat': now,
                'exp': now + current_app.config['JWT_ACCESS_TOKEN_EXPIRES']
            }
            
            access_token = jwt.encode(
                access_payload,
                current_app.config['JWT_SECRET_KEY'],
                algorithm=current_app.config['JWT_ALGORITHM']
            )
            
            logger.info(f'Refreshed access token for user {user.id}')
            
            return {'success': True, 'access_token': access_token}
            
        except Exception as e:
            logger.error(f'Token refresh error: {str(e)}')
            return {'success': False, 'message': 'Token refresh failed'}
    
    def blacklist_refresh_token(self, refresh_token, user_id):
        """
        Blacklist a refresh token.
        
        Args:
            refresh_token (str): Refresh token to blacklist
            user_id (int): User ID for verification
            
        Returns:
            dict: Result with success status and message
        """
        try:
            # Decode token to get JTI and expiration
            payload = self.decode_token(refresh_token)
            
            if not payload:
                return {'success': False, 'message': 'Invalid token'}
            
            if payload.get('user_id') != user_id:
                return {'success': False, 'message': 'Token user mismatch'}
            
            if payload.get('type') != 'refresh':
                return {'success': False, 'message': 'Invalid token type'}
            
            # Add to blacklist
            blacklist_entry = TokenBlacklist(
                jti=payload.get('jti'),
                user_id=user_id,
                token_type='refresh',
                expires_at=datetime.fromtimestamp(payload.get('exp'), timezone.utc)
            )
            
            db.session.add(blacklist_entry)
            db.session.commit()
            
            logger.info(f'Blacklisted refresh token for user {user_id}')
            
            return {'success': True, 'message': 'Token blacklisted successfully'}
            
        except Exception as e:
            logger.error(f'Token blacklist error: {str(e)}')
            return {'success': False, 'message': 'Failed to blacklist token'}
    
    def is_token_blacklisted(self, jti):
        """
        Check if a token JTI is blacklisted.
        
        Args:
            jti (str): JWT ID to check
            
        Returns:
            bool: True if blacklisted, False otherwise
        """
        if not jti:
            return True
        
        try:
            blacklisted = TokenBlacklist.query.filter_by(jti=jti).first()
            return blacklisted is not None
        except Exception as e:
            logger.error(f'Blacklist check error: {str(e)}')
            return True  # Err on the side of caution
    
    def revoke_all_user_tokens(self, user_id):
        """
        Revoke all tokens for a specific user.
        
        Args:
            user_id (int): User ID
            
        Returns:
            dict: Result with success status and revoked count
        """
        try:
            # This is a simplified approach - in production you might want to
            # track all issued tokens or use a different strategy
            
            # For now, we'll mark this timestamp and check it in token validation
            user = User.query.get(user_id)
            if not user:
                return {'success': False, 'message': 'User not found'}
            
            # Update user's updated_at timestamp
            # This can be used to invalidate tokens issued before this time
            user.updated_at = datetime.now(timezone.utc)
            db.session.commit()
            
            logger.info(f'Revoked all tokens for user {user_id}')
            
            return {'success': True, 'revoked_count': 'all', 'message': 'All tokens revoked'}
            
        except Exception as e:
            logger.error(f'Token revocation error: {str(e)}')
            return {'success': False, 'message': 'Failed to revoke tokens'}
    
    def validate_user_token(self, token, required_type='access'):
        """
        Validate token and return user if valid.
        
        Args:
            token (str): JWT token string
            required_type (str): Required token type ('access' or 'refresh')
            
        Returns:
            tuple: (user, payload) if valid, (None, None) if invalid
        """
        try:
            # Decode token
            payload = self.decode_token(token)
            
            if not payload:
                return None, None
            
            # Check token type
            if payload.get('type') != required_type:
                logger.warning(f'Invalid token type: expected {required_type}, got {payload.get("type")}')
                return None, None
            
            # Get user
            user = User.query.get(payload.get('user_id'))
            if not user:
                logger.warning(f'User not found for token: {payload.get("user_id")}')
                return None, None
            
            # Additional validation: check if token was issued before user's last update
            # This helps with the "revoke all tokens" functionality
            token_issued_at = datetime.fromtimestamp(payload.get('iat'), timezone.utc)
            if token_issued_at < user.updated_at:
                logger.info(f'Token issued before user update, considering invalid')
                return None, None
            
            return user, payload
            
        except Exception as e:
            logger.error(f'Token validation error: {str(e)}')
            return None, None
    
    def cleanup_expired_tokens(self):
        """
        Clean up expired blacklisted tokens.
        This should be run periodically as a maintenance task.
        """
        try:
            now = datetime.now(timezone.utc)
            expired_tokens = TokenBlacklist.query.filter(
                TokenBlacklist.expires_at < now
            ).all()
            
            count = len(expired_tokens)
            
            for token in expired_tokens:
                db.session.delete(token)
            
            db.session.commit()
            
            logger.info(f'Cleaned up {count} expired blacklisted tokens')
            
            return {'success': True, 'cleaned_count': count}
            
        except Exception as e:
            logger.error(f'Token cleanup error: {str(e)}')
            return {'success': False, 'message': 'Cleanup failed'}

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\services\cv_service.py -----
import os
import json
import logging
from datetime import datetime, timezone
from flask import current_app
from sqlalchemy import desc, asc, or_
from app.models import db, CV, CVStatus, User
from app import celery
import shutil

logger = logging.getLogger(__name__)


class CVService:
    """Service for CV management and operations."""
    
    def list_user_cvs(self, user_id, query_params):
        """
        List user's CVs with pagination and filtering.
        
        Args:
            user_id (int): User ID
            query_params (dict): Query parameters from request
            
        Returns:
            dict: Paginated CV results
        """
        try:
            # Base query
            query = CV.query.filter_by(user_id=user_id)
            
            # Apply filters
            if query_params.get('status'):
                try:
                    status = CVStatus(query_params['status'])
                    query = query.filter_by(status=status)
                except ValueError:
                    pass  # Invalid status, ignore filter
            
            if query_params.get('template_name'):
                query = query.filter_by(template_name=query_params['template_name'])
            
            if query_params.get('search'):
                search_term = f"%{query_params['search']}%"
                query = query.filter(
                    or_(
                        CV.title.ilike(search_term),
                        CV.user_data.ilike(search_term),
                        CV.job_description.ilike(search_term)
                    )
                )
            
            # Apply sorting
            sort_by = query_params.get('sort_by', 'created_at')
            sort_order = query_params.get('sort_order', 'desc')
            
            if hasattr(CV, sort_by):
                sort_column = getattr(CV, sort_by)
                if sort_order == 'asc':
                    query = query.order_by(asc(sort_column))
                else:
                    query = query.order_by(desc(sort_column))
            
            # Apply pagination
            page = query_params.get('page', 1)
            per_page = query_params.get('per_page', 10)
            
            pagination = query.paginate(
                page=page,
                per_page=per_page,
                error_out=False
            )
            
            return {
                'cvs': pagination.items,
                'page': pagination.page,
                'per_page': pagination.per_page,
                'total': pagination.total,
                'pages': pagination.pages,
                'has_next': pagination.has_next,
                'has_prev': pagination.has_prev
            }
            
        except Exception as e:
            logger.error(f'List user CVs error: {str(e)}')
            raise e
    
    def get_cv_by_uuid(self, cv_uuid, user_id=None):
        """
        Get CV by UUID with optional user verification.
        
        Args:
            cv_uuid (str): CV UUID
            user_id (int, optional): User ID for ownership verification
            
        Returns:
            CV: CV object or None if not found
        """
        try:
            query = CV.query.filter_by(uuid=cv_uuid)
            
            if user_id:
                query = query.filter_by(user_id=user_id)
            
            return query.first()
            
        except Exception as e:
            logger.error(f'Get CV by UUID error: {str(e)}')
            return None
    
    def delete_cv_files(self, cv):
        """
        Delete CV-related files from filesystem.
        
        Args:
            cv (CV): CV object
        """
        try:
            files_to_delete = []
            
            if cv.pdf_path and os.path.exists(cv.pdf_path):
                files_to_delete.append(cv.pdf_path)
            
            if cv.jpg_path and os.path.exists(cv.jpg_path):
                files_to_delete.append(cv.jpg_path)
            
            # Delete the entire CV directory if it exists
            cv_dir = os.path.dirname(cv.pdf_path) if cv.pdf_path else None
            if cv_dir and os.path.exists(cv_dir):
                try:
                    shutil.rmtree(cv_dir)
                    logger.info(f'Deleted CV directory: {cv_dir}')
                except Exception as e:
                    logger.warning(f'Failed to delete CV directory {cv_dir}: {str(e)}')
                    # Fall back to individual file deletion
                    for file_path in files_to_delete:
                        try:
                            os.remove(file_path)
                            logger.info(f'Deleted file: {file_path}')
                        except Exception as fe:
                            logger.warning(f'Failed to delete file {file_path}: {str(fe)}')
            
        except Exception as e:
            logger.error(f'Delete CV files error: {str(e)}')
    
    def get_task_status(self, task_id):
        """
        Get Celery task status.
        
        Args:
            task_id (str): Celery task ID
            
        Returns:
            dict: Task status information
        """
        try:
            if not task_id:
                return None
            
            # Get task result from Celery
            task_result = celery.AsyncResult(task_id)
            
            status_info = {
                'task_id': task_id,
                'state': task_result.state,
                'ready': task_result.ready(),
                'successful': task_result.successful(),
                'failed': task_result.failed()
            }
            
            # Add additional info based on state
            if task_result.state == 'PENDING':
                status_info['message'] = 'Task is waiting to be processed'
            elif task_result.state == 'PROGRESS':
                # Get progress info if available
                if hasattr(task_result, 'info') and task_result.info:
                    status_info.update(task_result.info)
            elif task_result.state == 'SUCCESS':
                status_info['result'] = task_result.result
            elif task_result.state == 'FAILURE':
                status_info['error'] = str(task_result.info)
            
            return status_info
            
        except Exception as e:
            logger.error(f'Get task status error: {str(e)}')
            return {
                'task_id': task_id,
                'state': 'UNKNOWN',
                'error': str(e)
            }
    
    def update_cv_status(self, cv_id, status, error_message=None, 
                        latex_code=None, pdf_path=None, jpg_path=None,
                        generation_time=None):
        """
        Update CV status and related fields.
        
        Args:
            cv_id (int): CV ID
            status (CVStatus): New status
            error_message (str, optional): Error message if failed
            latex_code (str, optional): Generated LaTeX code
            pdf_path (str, optional): Path to generated PDF
            jpg_path (str, optional): Path to generated JPG
            generation_time (float, optional): Generation time in seconds
        """
        try:
            cv = CV.query.get(cv_id)
            if not cv:
                logger.error(f'CV not found: {cv_id}')
                return False
            
            # Update status
            cv.status = status
            cv.updated_at = datetime.now(timezone.utc)
            
            # Update optional fields
            if error_message is not None:
                cv.error_message = error_message
            
            if latex_code is not None:
                cv.latex_code = latex_code
            
            if pdf_path is not None:
                cv.pdf_path = pdf_path
                # Get file size if file exists
                if os.path.exists(pdf_path):
                    cv.pdf_size = os.path.getsize(pdf_path)
            
            if jpg_path is not None:
                cv.jpg_path = jpg_path
            
            if generation_time is not None:
                cv.generation_time = generation_time
            
            db.session.commit()
            
            logger.info(f'Updated CV {cv_id} status to {status.value}')
            return True
            
        except Exception as e:
            logger.error(f'Update CV status error: {str(e)}')
            db.session.rollback()
            return False
    
    def get_user_cv_statistics(self, user_id):
        """
        Get CV statistics for a user.
        
        Args:
            user_id (int): User ID
            
        Returns:
            dict: CV statistics
        """
        try:
            total_cvs = CV.query.filter_by(user_id=user_id).count()
            
            successful_cvs = CV.query.filter_by(
                user_id=user_id, 
                status=CVStatus.SUCCESS
            ).count()
            
            failed_cvs = CV.query.filter_by(
                user_id=user_id, 
                status=CVStatus.FAILED
            ).count()
            
            processing_cvs = CV.query.filter(
                CV.user_id == user_id,
                CV.status.in_([CVStatus.PENDING, CVStatus.PROCESSING])
            ).count()
            
            # Get most used template
            from sqlalchemy import func
            template_stats = db.session.query(
                CV.template_name,
                func.count(CV.id).label('count')
            ).filter_by(user_id=user_id).group_by(CV.template_name).all()
            
            most_used_template = None
            if template_stats:
                most_used_template = max(template_stats, key=lambda x: x.count).template_name
            
            # Get average generation time
            avg_generation_time = db.session.query(
                func.avg(CV.generation_time)
            ).filter(
                CV.user_id == user_id,
                CV.generation_time.isnot(None)
            ).scalar()
            
            return {
                'total_cvs': total_cvs,
                'successful_cvs': successful_cvs,
                'failed_cvs': failed_cvs,
                'processing_cvs': processing_cvs,
                'success_rate': (successful_cvs / total_cvs * 100) if total_cvs > 0 else 0,
                'most_used_template': most_used_template,
                'average_generation_time': float(avg_generation_time) if avg_generation_time else None,
                'template_usage': {stat.template_name: stat.count for stat in template_stats}
            }
            
        except Exception as e:
            logger.error(f'Get user CV statistics error: {str(e)}')
            return {
                'total_cvs': 0,
                'successful_cvs': 0,
                'failed_cvs': 0,
                'processing_cvs': 0,
                'success_rate': 0,
                'most_used_template': None,
                'average_generation_time': None,
                'template_usage': {}
            }
    
    def search_cvs(self, user_id, search_query, filters=None, limit=10):
        """
        Search CVs with advanced filtering.
        
        Args:
            user_id (int): User ID
            search_query (str): Search query
            filters (dict, optional): Additional filters
            limit (int): Maximum results to return
            
        Returns:
            list: List of matching CVs
        """
        try:
            query = CV.query.filter_by(user_id=user_id)
            
            # Apply search query
            if search_query:
                search_term = f"%{search_query}%"
                query = query.filter(
                    or_(
                        CV.title.ilike(search_term),
                        CV.user_data.ilike(search_term),
                        CV.job_description.ilike(search_term),
                        CV.latex_code.ilike(search_term)
                    )
                )
            
            # Apply additional filters
            if filters:
                if 'status' in filters:
                    try:
                        status = CVStatus(filters['status'])
                        query = query.filter_by(status=status)
                    except ValueError:
                        pass
                
                if 'template_name' in filters:
                    query = query.filter_by(template_name=filters['template_name'])
                
                if 'date_from' in filters:
                    query = query.filter(CV.created_at >= filters['date_from'])
                
                if 'date_to' in filters:
                    query = query.filter(CV.created_at <= filters['date_to'])
            
            # Order by relevance (newest first for now)
            query = query.order_by(desc(CV.created_at))
            
            # Apply limit
            results = query.limit(limit).all()
            
            return [cv.to_dict() for cv in results]
            
        except Exception as e:
            logger.error(f'Search CVs error: {str(e)}')
            return []
    
    def bulk_delete_cvs(self, user_id, cv_ids):
        """
        Delete multiple CVs in bulk.
        
        Args:
            user_id (int): User ID
            cv_ids (list): List of CV IDs to delete
            
        Returns:
            dict: Result with deleted count and any errors
        """
        try:
            deleted_count = 0
            errors = []
            
            for cv_id in cv_ids:
                try:
                    cv = CV.query.filter_by(id=cv_id, user_id=user_id).first()
                    
                    if cv:
                        # Delete files
                        self.delete_cv_files(cv)
                        
                        # Delete record
                        db.session.delete(cv)
                        deleted_count += 1
                    else:
                        errors.append(f'CV {cv_id} not found or access denied')
                        
                except Exception as e:
                    errors.append(f'Failed to delete CV {cv_id}: {str(e)}')
            
            db.session.commit()
            
            logger.info(f'Bulk deleted {deleted_count} CVs for user {user_id}')
            
            return {
                'deleted_count': deleted_count,
                'errors': errors,
                'success': deleted_count > 0
            }
            
        except Exception as e:
            logger.error(f'Bulk delete CVs error: {str(e)}')
            db.session.rollback()
            return {
                'deleted_count': 0,
                'errors': [str(e)],
                'success': False
            }
    
    def cleanup_orphaned_files(self):
        """
        Clean up orphaned files that don't have corresponding CV records.
        This should be run periodically as a maintenance task.
        """
        try:
            upload_folder = current_app.config.get('UPLOAD_FOLDER', 'user_data')
            
            if not os.path.exists(upload_folder):
                return {'cleaned_count': 0, 'message': 'Upload folder does not exist'}
            
            cleaned_count = 0
            
            # Get all CV directories
            for item in os.listdir(upload_folder):
                item_path = os.path.join(upload_folder, item)
                
                if os.path.isdir(item_path) and item.startswith('cv_'):
                    # Extract UUID from directory name
                    try:
                        cv_uuid = item.replace('cv_', '')
                        
                        # Check if CV exists in database
                        cv = CV.query.filter_by(uuid=cv_uuid).first()
                        
                        if not cv:
                            # Orphaned directory, remove it
                            shutil.rmtree(item_path)
                            cleaned_count += 1
                            logger.info(f'Cleaned up orphaned directory: {item_path}')
                            
                    except Exception as e:
                        logger.warning(f'Error processing directory {item_path}: {str(e)}')
            
            return {
                'cleaned_count': cleaned_count,
                'message': f'Cleaned up {cleaned_count} orphaned files/directories'
            }
            
        except Exception as e:
            logger.error(f'Cleanup orphaned files error: {str(e)}')
            return {
                'cleaned_count': 0,
                'error': str(e)
            }

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\services\gemini_service.py -----
from google import genai
from google.genai import types
from pydantic import BaseModel
import os
import logging

logger = logging.getLogger(__name__)


class CVOutput(BaseModel):
    latex_code: str


def generate_cv_with_gemini(user_data, job_description, template_name):
    """
    Generate CV LaTeX code using Gemini AI.
    
    Args:
        user_data (dict): User information and details
        job_description (str): Job description to tailor CV for
        template_name (str): Template name to use
        
    Returns:
        str: Generated LaTeX code
    """
    try:
        # Configure Gemini
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        client = genai.Client(api_key=api_key)
        
        # Load template
        template_content = load_template(template_name)
        
        # Create comprehensive prompt
        prompt = f"""
You are a professional CV writer with expertise in creating compelling resumes that get interviews.

Create a professional CV based on the following information:

USER INFORMATION:
{format_user_data(user_data)}

JOB DESCRIPTION TO TAILOR FOR:
{job_description}

LATEX TEMPLATE TO CUSTOMIZE:
{template_content}

INSTRUCTIONS:
1. Customize the CV content to match the job requirements perfectly
2. Highlight relevant skills, experiences, and achievements
3. Use action verbs and quantify achievements where possible
4. Ensure the content flows naturally and professionally
5. Optimize for ATS (Applicant Tracking Systems)
6. Keep the LaTeX formatting intact and valid
7. Replace all placeholder content with the user's actual information
8. Tailor the professional summary to match the job requirements
9. Prioritize relevant experience and skills
10. Use industry-specific keywords from the job description

OUTPUT REQUIREMENTS:
- Return only valid LaTeX code that can be compiled
- Ensure all LaTeX syntax is correct
- No explanations or additional text outside the LaTeX code
- Make sure all special characters are properly escaped

Provide the complete LaTeX code in the latex_code field.
"""
        
        # Generate content with structured output
        response = client.models.generate_content(
            model="gemini-2.5-flash-preview-05-20",
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=CVOutput,
                thinking_config=types.ThinkingConfig(thinking_budget=0)
            )
        )
        
        # Use the parsed response
        cv_output: CVOutput = response.parsed
        latex_code = cv_output.latex_code
        
        # Validate the generated LaTeX
        if not latex_code or len(latex_code.strip()) < 100:
            raise Exception("Generated LaTeX code is too short or empty")
        
        if not latex_code.strip().startswith('\\documentclass'):
            raise Exception("Generated code doesn't appear to be valid LaTeX")
        
        logger.info(f"Successfully generated CV with Gemini for template {template_name}")
        return latex_code
        
    except Exception as e:
        logger.error(f"Failed to generate CV with Gemini: {str(e)}")
        # Return fallback template with user data
        return get_fallback_template(user_data, job_description, template_name)


def edit_cv_with_gemini(existing_latex, edit_instructions, user_data, job_description):
    """
    Edit existing CV LaTeX code using Gemini AI.
    
    Args:
        existing_latex (str): Current LaTeX code
        edit_instructions (str): Instructions for editing
        user_data (dict): User information
        job_description (str): Job description context
        
    Returns:
        str: Edited LaTeX code
    """
    try:
        # Configure Gemini
        api_key = os.environ.get('GEMINI_API_KEY')
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")
        
        client = genai.Client(api_key=api_key)
        
        # Create editing prompt
        prompt = f"""
You are a professional CV editor with expertise in improving resumes for better job matching.

CURRENT CV LATEX CODE:
{existing_latex}

USER INFORMATION (for context):
{format_user_data(user_data)}

JOB DESCRIPTION (for context):
{job_description}

EDITING INSTRUCTIONS:
{edit_instructions}

TASK:
Modify the existing LaTeX CV code according to the editing instructions while:

1. Maintaining the overall structure and formatting
2. Improving content quality and relevance
3. Ensuring the CV better matches the job requirements
4. Keeping all LaTeX syntax valid and compilable
5. Preserving the professional tone and readability
6. Making strategic improvements to highlight relevant skills
7. Optimizing for ATS compatibility
8. Following the specific editing instructions provided

IMPORTANT:
- Return only the complete modified LaTeX code
- Ensure all changes improve the CV's effectiveness
- Maintain proper LaTeX formatting and syntax
- Don't remove essential structural elements
- Keep the CV concise and focused

Provide the complete edited LaTeX code in the latex_code field.
"""
        
        # Generate edited content
        response = client.models.generate_content(
            model="gemini-2.5-flash-preview-05-20",
            contents=prompt,
            config=types.GenerateContentConfig(
                response_mime_type="application/json",
                response_schema=CVOutput,
                thinking_config=types.ThinkingConfig(thinking_budget=0)
            )
        )
        
        # Use the parsed response
        cv_output: CVOutput = response.parsed
        edited_latex = cv_output.latex_code
        
        # Validate the edited LaTeX
        if not edited_latex or len(edited_latex.strip()) < 100:
            raise Exception("Edited LaTeX code is too short or empty")
        
        if not edited_latex.strip().startswith('\\documentclass'):
            raise Exception("Edited code doesn't appear to be valid LaTeX")
        
        logger.info(f"Successfully edited CV with Gemini")
        return edited_latex
        
    except Exception as e:
        logger.error(f"Failed to edit CV with Gemini: {str(e)}")
        # Return original LaTeX if editing fails
        return existing_latex


def load_template(template_name):
    """
    Load LaTeX template from file.
    
    Args:
        template_name (str): Name of the template
        
    Returns:
        str: Template content
    """
    try:
        template_path = f'latex_templates/{template_name}.tex'
        
        if os.path.exists(template_path):
            with open(template_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            logger.warning(f"Template file not found: {template_path}")
            return get_basic_template()
            
    except Exception as e:
        logger.error(f"Error loading template {template_name}: {str(e)}")
        return get_basic_template()


def format_user_data(user_data):
    """
    Format user data for the prompt.
    
    Args:
        user_data (dict): User information
        
    Returns:
        str: Formatted user data string
    """
    try:
        formatted_lines = []
        
        # Basic information
        if user_data.get('name'):
            formatted_lines.append(f"Name: {user_data['name']}")
        if user_data.get('email'):
            formatted_lines.append(f"Email: {user_data['email']}")
        if user_data.get('phone'):
            formatted_lines.append(f"Phone: {user_data['phone']}")
        if user_data.get('location'):
            formatted_lines.append(f"Location: {user_data['location']}")
        if user_data.get('linkedin'):
            formatted_lines.append(f"LinkedIn: {user_data['linkedin']}")
        if user_data.get('website'):
            formatted_lines.append(f"Website: {user_data['website']}")
        
        # Professional summary
        if user_data.get('summary'):
            formatted_lines.append(f"\nProfessional Summary:\n{user_data['summary']}")
        
        # Experience
        if user_data.get('experience'):
            formatted_lines.append(f"\nExperience:\n{user_data['experience']}")
        
        # Skills
        if user_data.get('skills'):
            if isinstance(user_data['skills'], list):
                skills_str = ', '.join(user_data['skills'])
            else:
                skills_str = str(user_data['skills'])
            formatted_lines.append(f"\nSkills: {skills_str}")
        
        # Education
        if user_data.get('education'):
            formatted_lines.append(f"\nEducation:\n{user_data['education']}")
        
        # Projects
        if user_data.get('projects'):
            formatted_lines.append(f"\nProjects:\n{user_data['projects']}")
        
        # Certifications
        if user_data.get('certifications'):
            formatted_lines.append(f"\nCertifications:\n{user_data['certifications']}")
        
        # Languages
        if user_data.get('languages'):
            formatted_lines.append(f"\nLanguages: {user_data['languages']}")
        
        # Additional sections
        for key, value in user_data.items():
            if key not in ['name', 'email', 'phone', 'location', 'linkedin', 'website', 
                          'summary', 'experience', 'skills', 'education', 'projects', 
                          'certifications', 'languages'] and value:
                formatted_lines.append(f"\n{key.title()}: {value}")
        
        return '\n'.join(formatted_lines)
        
    except Exception as e:
        logger.error(f"Error formatting user data: {str(e)}")
        return str(user_data)


def get_basic_template():
    """
    Get basic LaTeX template as fallback.
    
    Returns:
        str: Basic LaTeX template
    """
    return r"""
\documentclass[11pt,a4paper,sans]{moderncv}
\moderncvstyle{classic}
\moderncvcolor{blue}
\usepackage[scale=0.75]{geometry}
\usepackage[utf8]{inputenc}

% Personal data
\name{[NAME]}{\space}
\title{[TITLE]}
\address{[ADDRESS]}{\space}{\space}
\phone[mobile]{[PHONE]}
\email{[EMAIL]}
\social[linkedin]{[LINKEDIN]}
\social[github]{[GITHUB]}

\begin{document}
\makecvtitle

\section{Professional Summary}
[SUMMARY]

\section{Experience}
\cventry{[YEAR]--[YEAR]}{[POSITION]}{[COMPANY]}{[LOCATION]}{}{%
[DESCRIPTION]
\begin{itemize}%
\item [ACHIEVEMENT_1]
\item [ACHIEVEMENT_2]
\item [ACHIEVEMENT_3]
\end{itemize}}

\section{Education}
\cventry{[YEAR]--[YEAR]}{[DEGREE]}{[INSTITUTION]}{[LOCATION]}{\textit{[GPA]}}{[DESCRIPTION]}

\section{Skills}
\cvitemwithcomment{Programming}{[PROGRAMMING_SKILLS]}{[LEVEL]}
\cvitemwithcomment{Technologies}{[TECH_SKILLS]}{[LEVEL]}
\cvitemwithcomment{Languages}{[LANGUAGES]}{[LEVEL]}

\section{Projects}
\cventry{[YEAR]}{[PROJECT_NAME]}{[TECH_STACK]}{}{}{%
[PROJECT_DESCRIPTION]
\begin{itemize}%
\item [FEATURE_1]
\item [FEATURE_2]
\end{itemize}}

\end{document}
"""


def get_fallback_template(user_data, job_description, template_name):
    """
    Get fallback template with basic user data substitution.
    
    Args:
        user_data (dict): User information
        job_description (str): Job description
        template_name (str): Template name
        
    Returns:
        str: LaTeX code with basic substitutions
    """
    try:
        template = get_basic_template()
        
        # Basic substitutions
        substitutions = {
            '[NAME]': user_data.get('name', 'Your Name'),
            '[TITLE]': extract_job_title(job_description) or 'Professional',
            '[EMAIL]': user_data.get('email', 'your.email@example.com'),
            '[PHONE]': user_data.get('phone', '+1-234-567-8900'),
            '[ADDRESS]': user_data.get('location', 'Your City, Country'),
            '[LINKEDIN]': user_data.get('linkedin', 'yourlinkedin'),
            '[GITHUB]': user_data.get('github', 'yourgithub'),
            '[SUMMARY]': user_data.get('summary', 'Professional with relevant experience.'),
            '[YEAR]': '2020',
            '[POSITION]': 'Position Title',
            '[COMPANY]': 'Company Name',
            '[LOCATION]': 'City, Country',
            '[DESCRIPTION]': user_data.get('experience', 'Professional experience description.'),
            '[ACHIEVEMENT_1]': 'Key achievement or responsibility',
            '[ACHIEVEMENT_2]': 'Another important accomplishment',
            '[ACHIEVEMENT_3]': 'Additional relevant experience',
            '[DEGREE]': user_data.get('education', 'Bachelor\'s Degree'),
            '[INSTITUTION]': 'University Name',
            '[GPA]': 'GPA',
            '[PROGRAMMING_SKILLS]': ', '.join(user_data.get('skills', ['Python', 'JavaScript'])) if isinstance(user_data.get('skills'), list) else str(user_data.get('skills', 'Programming Languages')),
            '[TECH_SKILLS]': 'React, Node.js, Docker',
            '[LANGUAGES]': 'English (Native), Spanish (Intermediate)',
            '[LEVEL]': 'Advanced',
            '[PROJECT_NAME]': 'Project Name',
            '[TECH_STACK]': 'Technology Stack',
            '[PROJECT_DESCRIPTION]': 'Project description and impact.',
            '[FEATURE_1]': 'Key feature or accomplishment',
            '[FEATURE_2]': 'Another important feature'
        }
        
        # Apply substitutions
        for placeholder, replacement in substitutions.items():
            template = template.replace(placeholder, replacement)
        
        logger.info(f"Generated fallback template for {template_name}")
        return template
        
    except Exception as e:
        logger.error(f"Error generating fallback template: {str(e)}")
        return get_basic_template()


def extract_job_title(job_description):
    """
    Extract job title from job description.
    
    Args:
        job_description (str): Job description text
        
    Returns:
        str: Extracted job title or None
    """
    try:
        # Simple extraction logic - look for common patterns
        job_description_lower = job_description.lower()
        
        common_titles = [
            'software engineer', 'software developer', 'full stack developer',
            'frontend developer', 'backend developer', 'data scientist',
            'product manager', 'project manager', 'marketing manager',
            'sales representative', 'business analyst', 'ux designer',
            'ui designer', 'graphic designer', 'content writer',
            'digital marketer', 'account manager', 'consultant'
        ]
        
        for title in common_titles:
            if title in job_description_lower:
                return title.title()
        
        # If no common title found, try to extract from first few words
        words = job_description.split()[:10]
        for i, word in enumerate(words):
            if word.lower() in ['engineer', 'developer', 'manager', 'analyst', 'designer', 'specialist']:
                if i > 0:
                    return ' '.join(words[max(0, i-2):i+1]).title()
                else:
                    return word.title()
        
        return None
        
    except Exception as e:
        logger.error(f"Error extracting job title: {str(e)}")
        return None


def validate_latex_code(latex_code):
    """
    Basic validation for LaTeX code.
    
    Args:
        latex_code (str): LaTeX code to validate
        
    Returns:
        tuple: (is_valid, error_message)
    """
    try:
        if not latex_code or not latex_code.strip():
            return False, "LaTeX code is empty"
        
        if not latex_code.strip().startswith('\\documentclass'):
            return False, "LaTeX code must start with \\documentclass"
        
        if '\\begin{document}' not in latex_code:
            return False, "LaTeX code must contain \\begin{document}"
        
        if '\\end{document}' not in latex_code:
            return False, "LaTeX code must contain \\end{document}"
        
        # Check for balanced braces (basic check)
        open_braces = latex_code.count('{')
        close_braces = latex_code.count('}')
        
        if open_braces != close_braces:
            return False, f"Unbalanced braces: {open_braces} opening, {close_braces} closing"
        
        return True, "LaTeX code appears valid"
        
    except Exception as e:
        return False, f"Error validating LaTeX: {str(e)}"

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\services\latex_service.py -----
import os
import subprocess
import logging
import tempfile
import shutil
from flask import current_app
from PIL import Image, ImageDraw, ImageFont
import fitz  # PyMuPDF
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter, A4
from io import BytesIO

logger = logging.getLogger(__name__)


def compile_latex_to_pdf(cv_uuid, latex_code, user_tier):
    """
    Compile LaTeX code to PDF with enhanced error handling and fallbacks.
    
    Args:
        cv_uuid (str): CV UUID for file organization
        latex_code (str): LaTeX source code
        user_tier (str): User subscription tier
        
    Returns:
        tuple: (pdf_path, jpg_path) or raises exception on failure
    """
    try:
        # Create user directory
        base_dir = current_app.config.get('UPLOAD_FOLDER', 'user_data')
        user_dir = os.path.join(base_dir, f'cv_{cv_uuid}')
        os.makedirs(user_dir, exist_ok=True)
        
        # Write LaTeX file
        tex_file = os.path.join(user_dir, 'cv.tex')
        with open(tex_file, 'w', encoding='utf-8') as f:
            f.write(latex_code)
        
        logger.info(f'Wrote LaTeX file for CV {cv_uuid}')
        
        # Try to compile with pdflatex
        pdf_path, jpg_path = _compile_with_pdflatex(user_dir, user_tier, cv_uuid)
        
        if pdf_path and os.path.exists(pdf_path):
            logger.info(f'Successfully compiled LaTeX to PDF for CV {cv_uuid}')
            return pdf_path, jpg_path
        else:
            # Fallback to creating PDF from LaTeX content
            logger.warning(f'pdflatex failed for CV {cv_uuid}, using fallback method')
            return _create_fallback_pdf(user_dir, latex_code, user_tier, cv_uuid)
        
    except Exception as e:
        logger.error(f'LaTeX compilation error for CV {cv_uuid}: {str(e)}')
        # Last resort: create dummy files
        return _create_dummy_files(user_dir, user_tier, cv_uuid)


def _compile_with_pdflatex(user_dir, user_tier, cv_uuid):
    """
    Compile LaTeX using pdflatex system command.
    
    Args:
        user_dir (str): Directory containing LaTeX files
        user_tier (str): User subscription tier
        cv_uuid (str): CV UUID
        
    Returns:
        tuple: (pdf_path, jpg_path) or (None, None) if failed
    """
    try:
        # Check if pdflatex is available
        result = subprocess.run(['pdflatex', '--version'], 
                              capture_output=True, check=True, timeout=10)
        logger.info('pdflatex is available')
        
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        logger.warning('pdflatex not available or timed out')
        return None, None
    
    try:
        # Compile LaTeX with timeout
        cmd = [
            'pdflatex',
            '-interaction=nonstopmode',
            '-output-directory', user_dir,
            'cv.tex'
        ]
        
        # Run compilation with timeout
        result = subprocess.run(
            cmd, 
            cwd=user_dir, 
            capture_output=True, 
            text=True, 
            timeout=30
        )
        
        pdf_path = os.path.join(user_dir, 'cv.pdf')
        
        if result.returncode != 0:
            logger.error(f'pdflatex compilation failed: {result.stderr}')
            return None, None
        
        if not os.path.exists(pdf_path):
            logger.error('PDF file was not created despite successful compilation')
            return None, None
        
        # Create JPG preview for free users
        jpg_path = None
        if user_tier == 'free':
            jpg_path = _create_jpg_preview(pdf_path, user_dir)
        
        return pdf_path, jpg_path
        
    except subprocess.TimeoutExpired:
        logger.error('pdflatex compilation timed out')
        return None, None
    except Exception as e:
        logger.error(f'pdflatex compilation error: {str(e)}')
        return None, None


def _create_jpg_preview(pdf_path, user_dir):
    """
    Create JPG preview from PDF first page.
    
    Args:
        pdf_path (str): Path to PDF file
        user_dir (str): User directory
        
    Returns:
        str: Path to JPG file or None if failed
    """
    try:
        jpg_path = os.path.join(user_dir, 'cv.jpg')
        
        # Use PyMuPDF to convert PDF to image
        doc = fitz.open(pdf_path)
        page = doc.load_page(0)  # first page
        
        # Render page to image with high quality
        mat = fitz.Matrix(2.0, 2.0)  # 2x zoom for better quality
        pix = page.get_pixmap(matrix=mat)
        
        # Convert to PIL Image and save as JPG
        img_data = pix.tobytes("ppm")
        img = Image.open(BytesIO(img_data))
        
        # Resize if too large
        if img.width > 1200 or img.height > 1600:
            img.thumbnail((1200, 1600), Image.Resampling.LANCZOS)
        
        img.save(jpg_path, 'JPEG', quality=85, optimize=True)
        doc.close()
        
        logger.info(f'Created JPG preview: {jpg_path}')
        return jpg_path
        
    except Exception as e:
        logger.warning(f'Failed to create JPG preview: {str(e)}')
        return None


def _create_fallback_pdf(user_dir, latex_code, user_tier, cv_uuid):
    """
    Create PDF using reportlab as fallback when LaTeX compilation fails.
    
    Args:
        user_dir (str): User directory
        latex_code (str): LaTeX source code
        user_tier (str): User subscription tier
        cv_uuid (str): CV UUID
        
    Returns:
        tuple: (pdf_path, jpg_path)
    """
    try:
        pdf_path = os.path.join(user_dir, 'cv.pdf')
        
        # Extract basic information from LaTeX
        cv_content = _extract_content_from_latex(latex_code)
        
        # Create PDF using reportlab
        c = canvas.Canvas(pdf_path, pagesize=A4)
        width, height = A4
        
        # Set up fonts and spacing
        title_font = "Helvetica-Bold"
        header_font = "Helvetica-Bold"
        body_font = "Helvetica"
        
        y_position = height - 50
        
        # Title
        if cv_content.get('name'):
            c.setFont(title_font, 24)
            c.drawString(50, y_position, cv_content['name'])
            y_position -= 40
        
        # Contact information
        c.setFont(body_font, 12)
        contact_info = []
        if cv_content.get('email'):
            contact_info.append(f"Email: {cv_content['email']}")
        if cv_content.get('phone'):
            contact_info.append(f"Phone: {cv_content['phone']}")
        
        if contact_info:
            c.drawString(50, y_position, " | ".join(contact_info))
            y_position -= 30
        
        # Professional Summary
        if cv_content.get('summary'):
            c.setFont(header_font, 14)
            c.drawString(50, y_position, "Professional Summary")
            y_position -= 20
            
            c.setFont(body_font, 11)
            _draw_wrapped_text(c, cv_content['summary'], 50, y_position, width - 100)
            y_position -= 60
        
        # Experience
        if cv_content.get('experience'):
            c.setFont(header_font, 14)
            c.drawString(50, y_position, "Experience")
            y_position -= 20
            
            c.setFont(body_font, 11)
            _draw_wrapped_text(c, cv_content['experience'], 50, y_position, width - 100)
            y_position -= 60
        
        # Skills
        if cv_content.get('skills'):
            c.setFont(header_font, 14)
            c.drawString(50, y_position, "Skills")
            y_position -= 20
            
            c.setFont(body_font, 11)
            skills_text = cv_content['skills'] if isinstance(cv_content['skills'], str) else ', '.join(cv_content['skills'])
            _draw_wrapped_text(c, skills_text, 50, y_position, width - 100)
            y_position -= 60
        
        # Education
        if cv_content.get('education'):
            c.setFont(header_font, 14)
            c.drawString(50, y_position, "Education")
            y_position -= 20
            
            c.setFont(body_font, 11)
            _draw_wrapped_text(c, cv_content['education'], 50, y_position, width - 100)
        
        # Add watermark for fallback
        c.setFont("Helvetica", 8)
        c.setFillGray(0.7)
        c.drawString(50, 30, f"Generated by MorphCV - CV ID: {cv_uuid[:8]}")
        
        c.save()
        
        # Create JPG preview for free users
        jpg_path = None
        if user_tier == 'free':
            jpg_path = _create_jpg_preview(pdf_path, user_dir)
        
        logger.info(f'Created fallback PDF for CV {cv_uuid}')
        return pdf_path, jpg_path
        
    except Exception as e:
        logger.error(f'Fallback PDF creation failed: {str(e)}')
        return _create_dummy_files(user_dir, user_tier, cv_uuid)


def _extract_content_from_latex(latex_code):
    """
    Extract basic content from LaTeX code for fallback PDF generation.
    
    Args:
        latex_code (str): LaTeX source code
        
    Returns:
        dict: Extracted content
    """
    content = {}
    
    try:
        # Simple regex-based extraction
        import re
        
        # Extract name (look for common patterns)
        name_patterns = [
            r'\\name\{([^}]+)\}',
            r'\\textbf\{([^}]+)\}.*(?:CV|Resume)',
            r'\\LARGE\s*\\textbf\{([^}]+)\}',
            r'\\huge\s*([^\\]+)'
        ]
        
        for pattern in name_patterns:
            match = re.search(pattern, latex_code, re.IGNORECASE)
            if match:
                content['name'] = match.group(1).strip()
                break
        
        # Extract email
        email_match = re.search(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', latex_code)
        if email_match:
            content['email'] = email_match.group(1)
        
        # Extract phone (basic pattern)
        phone_match = re.search(r'(\+?[\d\s\-\(\)]{10,})', latex_code)
        if phone_match:
            content['phone'] = phone_match.group(1).strip()
        
        # Extract sections
        sections = ['summary', 'experience', 'skills', 'education']
        for section in sections:
            pattern = rf'\\section\*?\{{[^}}]*{section}[^}}]*\}}(.*?)(?=\\section|\Z)'
            match = re.search(pattern, latex_code, re.IGNORECASE | re.DOTALL)
            if match:
                # Clean up LaTeX commands
                text = match.group(1)
                text = re.sub(r'\\[a-zA-Z]+\{([^}]*)\}', r'\1', text)
                text = re.sub(r'\\[a-zA-Z]+', '', text)
                text = re.sub(r'\{|\}', '', text)
                text = re.sub(r'\s+', ' ', text).strip()
                if text:
                    content[section] = text
        
        return content
        
    except Exception as e:
        logger.warning(f'Content extraction failed: {str(e)}')
        return {'name': 'CV Generated by MorphCV'}


def _draw_wrapped_text(canvas_obj, text, x, y, max_width):
    """
    Draw text with word wrapping.
    
    Args:
        canvas_obj: ReportLab canvas object
        text (str): Text to draw
        x (int): X coordinate
        y (int): Y coordinate
        max_width (int): Maximum width for text
    """
    try:
        words = text.split()
        lines = []
        current_line = []
        
        for word in words:
            test_line = ' '.join(current_line + [word])
            if canvas_obj.stringWidth(test_line) <= max_width:
                current_line.append(word)
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                    current_line = [word]
                else:
                    lines.append(word)  # Word is too long, add anyway
        
        if current_line:
            lines.append(' '.join(current_line))
        
        for i, line in enumerate(lines):
            canvas_obj.drawString(x, y - (i * 15), line)
            
    except Exception as e:
        logger.warning(f'Text wrapping failed: {str(e)}')
        canvas_obj.drawString(x, y, text[:100] + '...' if len(text) > 100 else text)


def _create_dummy_files(user_dir, user_tier, cv_uuid):
    """
    Create dummy files as last resort when all other methods fail.
    
    Args:
        user_dir (str): User directory
        user_tier (str): User subscription tier
        cv_uuid (str): CV UUID
        
    Returns:
        tuple: (pdf_path, jpg_path)
    """
    try:
        # Create simple PDF using reportlab
        pdf_path = os.path.join(user_dir, 'cv.pdf')
        
        c = canvas.Canvas(pdf_path, pagesize=A4)
        width, height = A4
        
        # Simple placeholder content
        c.setFont("Helvetica-Bold", 24)
        c.drawString(50, height - 100, "CV Generation in Progress")
        
        c.setFont("Helvetica", 14)
        c.drawString(50, height - 150, "Your CV is being processed using our AI system.")
        c.drawString(50, height - 180, "Please try downloading again in a few moments.")
        
        c.setFont("Helvetica", 10)
        c.drawString(50, 50, f"CV ID: {cv_uuid}")
        c.drawString(50, 35, "Generated by MorphCV - Professional CV Generation")
        
        c.save()
        
        # Create JPG for free users
        jpg_path = None
        if user_tier == 'free':
            jpg_path = os.path.join(user_dir, 'cv.jpg')
            
            # Create simple image using PIL
            img = Image.new('RGB', (600, 800), color='white')
            draw = ImageDraw.Draw(img)
            
            try:
                # Try to use a better font
                font_large = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf", 36)
                font_small = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", 18)
            except:
                # Fallback to default font
                font_large = ImageFont.load_default()
                font_small = ImageFont.load_default()
            
            draw.text((50, 100), "CV Generation", fill='black', font=font_large)
            draw.text((50, 200), "Your CV is being processed", fill='black', font=font_small)
            draw.text((50, 250), "Please try downloading again", fill='black', font=font_small)
            draw.text((50, 300), "in a few moments.", fill='black', font=font_small)
            
            draw.text((50, 700), f"CV ID: {cv_uuid[:8]}", fill='gray', font=font_small)
            
            img.save(jpg_path, 'JPEG', quality=80)
        
        logger.info(f'Created dummy files for CV {cv_uuid}')
        return pdf_path, jpg_path
        
    except Exception as e:
        logger.error(f'Failed to create dummy files: {str(e)}')
        raise Exception(f"Complete PDF generation failure: {str(e)}")


def cleanup_temp_files(user_dir):
    """
    Clean up temporary LaTeX compilation files.
    
    Args:
        user_dir (str): User directory path
    """
    try:
        temp_extensions = ['.aux', '.log', '.out', '.fdb_latexmk', '.fls', '.synctex.gz']
        
        for file in os.listdir(user_dir):
            if any(file.endswith(ext) for ext in temp_extensions):
                file_path = os.path.join(user_dir, file)
                os.remove(file_path)
                logger.debug(f'Cleaned up temp file: {file_path}')
                
    except Exception as e:
        logger.warning(f'Failed to cleanup temp files: {str(e)}')


def validate_latex_code(latex_code):
    """
    Validate LaTeX code for basic syntax errors.
    
    Args:
        latex_code (str): LaTeX source code
        
    Returns:
        tuple: (is_valid, error_messages)
    """
    errors = []
    
    try:
        if not latex_code or not latex_code.strip():
            errors.append("LaTeX code is empty")
            return False, errors
        
        # Check for document class
        if '\\documentclass' not in latex_code:
            errors.append("Missing \\documentclass declaration")
        
        # Check for document environment
        if '\\begin{document}' not in latex_code:
            errors.append("Missing \\begin{document}")
        
        if '\\end{document}' not in latex_code:
            errors.append("Missing \\end{document}")
        
        # Check for balanced braces
        open_braces = latex_code.count('{')
        close_braces = latex_code.count('}')
        
        if open_braces != close_braces:
            errors.append(f"Unbalanced braces: {open_braces} opening, {close_braces} closing")
        
        # Check for dangerous commands (security)
        dangerous_commands = ['\\write18', '\\input', '\\include', '\\openin', '\\openout']
        for cmd in dangerous_commands:
            if cmd in latex_code:
                errors.append(f"Potentially dangerous command detected: {cmd}")
        
        return len(errors) == 0, errors
        
    except Exception as e:
        errors.append(f"Validation error: {str(e)}")
        return False, errors

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\services\payment_service.py -----
import stripe
import logging
from datetime import datetime, timezone
from flask import current_app
from app.models import db, User, UserTier

logger = logging.getLogger(__name__)


class PaymentService:
    """Service for handling Stripe payments and subscriptions."""
    
    def __init__(self):
        """Initialize Stripe with API key."""
        stripe.api_key = current_app.config.get('STRIPE_SECRET_KEY')
    
    def create_customer(self, email, name=None, user_id=None):
        """
        Create a new Stripe customer.
        
        Args:
            email (str): Customer email
            name (str, optional): Customer name
            user_id (int, optional): Internal user ID
            
        Returns:
            dict: Stripe customer object or None if failed
        """
        try:
            customer_data = {
                'email': email,
                'metadata': {}
            }
            
            if name:
                customer_data['name'] = name
            
            if user_id:
                customer_data['metadata']['user_id'] = str(user_id)
            
            customer = stripe.Customer.create(**customer_data)
            
            logger.info(f'Created Stripe customer {customer.id} for email {email}')
            return customer
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error creating customer: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error creating customer: {str(e)}')
            return None
    
    def get_customer_subscription(self, customer_id):
        """
        Get customer's active subscription.
        
        Args:
            customer_id (str): Stripe customer ID
            
        Returns:
            dict: Subscription details or None if no active subscription
        """
        try:
            subscriptions = stripe.Subscription.list(
                customer=customer_id,
                status='active',
                limit=1
            )
            
            if subscriptions.data:
                subscription = subscriptions.data[0]
                return {
                    'id': subscription.id,
                    'status': subscription.status,
                    'current_period_start': datetime.fromtimestamp(
                        subscription.current_period_start, timezone.utc
                    ).isoformat(),
                    'current_period_end': datetime.fromtimestamp(
                        subscription.current_period_end, timezone.utc
                    ).isoformat(),
                    'cancel_at_period_end': subscription.cancel_at_period_end,
                    'plan_name': subscription.items.data[0].price.nickname,
                    'amount': subscription.items.data[0].price.unit_amount,
                    'currency': subscription.items.data[0].price.currency,
                    'interval': subscription.items.data[0].price.recurring.interval
                }
            
            return None
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error getting subscription: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error getting subscription: {str(e)}')
            return None
    
    def create_checkout_session(self, customer_id, price_id, success_url, cancel_url, user_id=None):
        """
        Create a Stripe checkout session for subscription.
        
        Args:
            customer_id (str): Stripe customer ID
            price_id (str): Stripe price ID
            success_url (str): Success redirect URL
            cancel_url (str): Cancel redirect URL
            user_id (int, optional): Internal user ID
            
        Returns:
            dict: Checkout session object or None if failed
        """
        try:
            session_data = {
                'customer': customer_id,
                'payment_method_types': ['card'],
                'line_items': [{
                    'price': price_id,
                    'quantity': 1,
                }],
                'mode': 'subscription',
                'success_url': success_url,
                'cancel_url': cancel_url,
                'metadata': {}
            }
            
            if user_id:
                session_data['metadata']['user_id'] = str(user_id)
            
            session = stripe.checkout.Session.create(**session_data)
            
            logger.info(f'Created checkout session {session.id} for customer {customer_id}')
            return session
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error creating checkout session: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error creating checkout session: {str(e)}')
            return None
    
    def create_customer_portal_session(self, customer_id, return_url):
        """
        Create a Stripe customer portal session.
        
        Args:
            customer_id (str): Stripe customer ID
            return_url (str): Return URL after portal session
            
        Returns:
            dict: Portal session object or None if failed
        """
        try:
            session = stripe.billing_portal.Session.create(
                customer=customer_id,
                return_url=return_url,
            )
            
            logger.info(f'Created customer portal session for customer {customer_id}')
            return session
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error creating portal session: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error creating portal session: {str(e)}')
            return None
    
    def get_subscription_prices(self):
        """
        Get available subscription prices from Stripe.
        
        Returns:
            list: List of available prices or None if failed
        """
        try:
            prices = stripe.Price.list(
                active=True,
                type='recurring',
                limit=20
            )
            
            formatted_prices = []
            for price in prices.data:
                # Only include prices with products that have metadata indicating they're for CV generation
                try:
                    product = stripe.Product.retrieve(price.product)
                    if product.metadata.get('service_type') == 'cv_generation':
                        formatted_prices.append({
                            'id': price.id,
                            'product_id': price.product,
                            'product_name': product.name,
                            'product_description': product.description,
                            'amount': price.unit_amount,
                            'currency': price.currency,
                            'interval': price.recurring.interval,
                            'interval_count': price.recurring.interval_count,
                            'nickname': price.nickname,
                            'tier': product.metadata.get('tier', 'unknown')
                        })
                except Exception as e:
                    logger.warning(f'Error processing price {price.id}: {str(e)}')
                    continue
            
            # Sort by amount
            formatted_prices.sort(key=lambda x: x['amount'])
            
            logger.info(f'Retrieved {len(formatted_prices)} subscription prices')
            return formatted_prices
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error getting prices: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error getting prices: {str(e)}')
            return None
    
    def cancel_subscription(self, subscription_id, cancel_at_period_end=True, reason=None):
        """
        Cancel a subscription.
        
        Args:
            subscription_id (str): Stripe subscription ID
            cancel_at_period_end (bool): Whether to cancel at period end
            reason (str, optional): Cancellation reason
            
        Returns:
            dict: Updated subscription object or None if failed
        """
        try:
            if cancel_at_period_end:
                subscription = stripe.Subscription.modify(
                    subscription_id,
                    cancel_at_period_end=True,
                    metadata={'cancellation_reason': reason} if reason else {}
                )
            else:
                subscription = stripe.Subscription.delete(subscription_id)
            
            logger.info(f'Cancelled subscription {subscription_id}')
            return subscription
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error cancelling subscription: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error cancelling subscription: {str(e)}')
            return None
    
    def reactivate_subscription(self, subscription_id):
        """
        Reactivate a cancelled subscription.
        
        Args:
            subscription_id (str): Stripe subscription ID
            
        Returns:
            dict: Updated subscription object or None if failed
        """
        try:
            subscription = stripe.Subscription.modify(
                subscription_id,
                cancel_at_period_end=False
            )
            
            logger.info(f'Reactivated subscription {subscription_id}')
            return subscription
            
        except stripe.error.StripeError as e:
            logger.error(f'Stripe error reactivating subscription: {str(e)}')
            return None
        except Exception as e:
            logger.error(f'Error reactivating subscription: {str(e)}')
            return None
    
    def handle_subscription_created(self, subscription_data):
        """
        Handle subscription.created webhook event.
        
        Args:
            subscription_data (dict): Stripe subscription object
        """
        try:
            customer_id = subscription_data['customer']
            subscription_id = subscription_data['id']
            
            # Find user by Stripe customer ID
            user = User.query.filter_by(stripe_customer_id=customer_id).first()
            if not user:
                logger.error(f'User not found for customer {customer_id}')
                return
            
            # Update user subscription info
            user.subscription_id = subscription_id
            user.subscription_status = subscription_data['status']
            user.subscription_current_period_end = datetime.fromtimestamp(
                subscription_data['current_period_end'], timezone.utc
            )
            
            # Update user tier based on subscription
            tier = self._get_tier_from_subscription(subscription_data)
            if tier:
                user.user_tier = tier
                if tier in [UserTier.PRO, UserTier.ENTERPRISE]:
                    user.generations_left = 999  # Unlimited for paid tiers
            
            db.session.commit()
            
            logger.info(f'Updated user {user.id} with new subscription {subscription_id}')
            
        except Exception as e:
            logger.error(f'Error handling subscription created: {str(e)}')
            db.session.rollback()
    
    def handle_subscription_updated(self, subscription_data):
        """
        Handle subscription.updated webhook event.
        
        Args:
            subscription_data (dict): Stripe subscription object
        """
        try:
            subscription_id = subscription_data['id']
            
            # Find user by subscription ID
            user = User.query.filter_by(subscription_id=subscription_id).first()
            if not user:
                logger.error(f'User not found for subscription {subscription_id}')
                return
            
            # Update subscription info
            user.subscription_status = subscription_data['status']
            user.subscription_current_period_end = datetime.fromtimestamp(
                subscription_data['current_period_end'], timezone.utc
            )
            
            # Update tier if subscription changed
            tier = self._get_tier_from_subscription(subscription_data)
            if tier:
                user.user_tier = tier
                if tier in [UserTier.PRO, UserTier.ENTERPRISE]:
                    user.generations_left = 999  # Unlimited for paid tiers
            
            db.session.commit()
            
            logger.info(f'Updated subscription for user {user.id}')
            
        except Exception as e:
            logger.error(f'Error handling subscription updated: {str(e)}')
            db.session.rollback()
    
    def handle_subscription_cancelled(self, subscription_data):
        """
        Handle subscription.deleted webhook event.
        
        Args:
            subscription_data (dict): Stripe subscription object
        """
        try:
            subscription_id = subscription_data['id']
            
            # Find user by subscription ID
            user = User.query.filter_by(subscription_id=subscription_id).first()
            if not user:
                logger.error(f'User not found for subscription {subscription_id}')
                return
            
            # Downgrade user to free tier
            user.user_tier = UserTier.FREE
            user.subscription_status = 'cancelled'
            user.subscription_id = None
            user.subscription_current_period_end = None
            user.generations_left = 2  # Reset to free tier limit
            
            db.session.commit()
            
            logger.info(f'Downgraded user {user.id} to free tier after subscription cancellation')
            
        except Exception as e:
            logger.error(f'Error handling subscription cancelled: {str(e)}')
            db.session.rollback()
    
    def handle_payment_succeeded(self, invoice_data):
        """
        Handle invoice.payment_succeeded webhook event.
        
        Args:
            invoice_data (dict): Stripe invoice object
        """
        try:
            customer_id = invoice_data['customer']
            subscription_id = invoice_data['subscription']
            
            # Find user by customer ID
            user = User.query.filter_by(stripe_customer_id=customer_id).first()
            if not user:
                logger.error(f'User not found for customer {customer_id}')
                return
            
            # Reset generation count for paid tiers on successful payment
            if user.user_tier in [UserTier.PRO, UserTier.ENTERPRISE]:
                user.generations_left = 999  # Unlimited
            
            db.session.commit()
            
            logger.info(f'Processed successful payment for user {user.id}')
            
        except Exception as e:
            logger.error(f'Error handling payment succeeded: {str(e)}')
            db.session.rollback()
    
    def handle_payment_failed(self, invoice_data):
        """
        Handle invoice.payment_failed webhook event.
        
        Args:
            invoice_data (dict): Stripe invoice object
        """
        try:
            customer_id = invoice_data['customer']
            
            # Find user by customer ID
            user = User.query.filter_by(stripe_customer_id=customer_id).first()
            if not user:
                logger.error(f'User not found for customer {customer_id}')
                return
            
            # Log the failed payment for monitoring
            logger.warning(f'Payment failed for user {user.id}, customer {customer_id}')
            
            # Note: Don't immediately downgrade user - Stripe will retry payments
            # and send subscription.updated/deleted events if needed
            
        except Exception as e:
            logger.error(f'Error handling payment failed: {str(e)}')
    
    def handle_customer_created(self, customer_data):
        """
        Handle customer.created webhook event.
        
        Args:
            customer_data (dict): Stripe customer object
        """
        try:
            customer_id = customer_data['id']
            email = customer_data['email']
            
            # Find user by email and update customer ID if not set
            user = User.query.filter_by(email=email).first()
            if user and not user.stripe_customer_id:
                user.stripe_customer_id = customer_id
                db.session.commit()
                logger.info(f'Linked customer {customer_id} to user {user.id}')
            
        except Exception as e:
            logger.error(f'Error handling customer created: {str(e)}')
            db.session.rollback()
    
    def handle_customer_updated(self, customer_data):
        """
        Handle customer.updated webhook event.
        
        Args:
            customer_data (dict): Stripe customer object
        """
        try:
            customer_id = customer_data['id']
            
            # Find user by customer ID
            user = User.query.filter_by(stripe_customer_id=customer_id).first()
            if user:
                # Update user info if email changed
                if customer_data.get('email') and customer_data['email'] != user.email:
                    user.email = customer_data['email']
                
                if customer_data.get('name') and customer_data['name'] != user.name:
                    user.name = customer_data['name']
                
                db.session.commit()
                logger.info(f'Updated customer info for user {user.id}')
            
        except Exception as e:
            logger.error(f'Error handling customer updated: {str(e)}')
            db.session.rollback()
    
    def _get_tier_from_subscription(self, subscription_data):
        """
        Determine user tier from subscription data.
        
        Args:
            subscription_data (dict): Stripe subscription object
            
        Returns:
            UserTier: User tier enum value or None
        """
        try:
            # Get the price ID from the subscription
            if subscription_data.get('items') and subscription_data['items']['data']:
                price_id = subscription_data['items']['data'][0]['price']['id']
                
                # Get the product to check metadata
                price = stripe.Price.retrieve(price_id)
                product = stripe.Product.retrieve(price.product)
                
                # Check product metadata for tier information
                tier_mapping = {
                    'pro': UserTier.PRO,
                    'enterprise': UserTier.ENTERPRISE,
                    'premium': UserTier.PRO,
                    'business': UserTier.ENTERPRISE
                }
                
                tier_name = product.metadata.get('tier', '').lower()
                return tier_mapping.get(tier_name, UserTier.PRO)  # Default to PRO for paid
            
            return UserTier.PRO  # Default for any paid subscription
            
        except Exception as e:
            logger.error(f'Error determining tier from subscription: {str(e)}')
            return UserTier.PRO  # Default fallback

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\tasks\cv_tasks.py -----
import time
import json
import logging
from celery import current_task
from app import celery
from app.models import CVStatus
from app.services.cv_service import CVService
from app.services.gemini_service import generate_cv_with_gemini, edit_cv_with_gemini
from app.services.latex_service import compile_latex_to_pdf
from sqlalchemy import text


logger = logging.getLogger(__name__)


@celery.task(bind=True, name='generate_cv_task')
def generate_cv_task(self, cv_id, user_data, job_description, template_name, user_tier):
    """
    Celery task for generating CV using Gemini and LaTeX.
    
    Args:
        cv_id (int): CV database ID
        user_data (dict): User information
        job_description (str): Job description to tailor CV for
        template_name (str): LaTeX template to use
        user_tier (str): User subscription tier
    """
    cv_service = CVService()
    start_time = time.time()
    
    try:
        logger.info(f'Starting CV generation for CV {cv_id}')
        
        # Update task progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Generating CV content with AI', 'progress': 20}
        )
        
        # Step 1: Generate LaTeX code using Gemini
        latex_code = generate_cv_with_gemini(user_data, job_description, template_name)
        
        if not latex_code:
            raise Exception("Failed to generate LaTeX code with Gemini")
        
        logger.info(f'Generated LaTeX code for CV {cv_id}')
        
        # Update progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Compiling PDF document', 'progress': 60}
        )
        
        # Step 2: Compile LaTeX to PDF
        from app.models import CV
        cv = CV.query.get(cv_id)
        if not cv:
            raise Exception(f"CV {cv_id} not found in database")
        
        pdf_path, jpg_path = compile_latex_to_pdf(cv.uuid, latex_code, user_tier)
        
        logger.info(f'Compiled PDF for CV {cv_id}')
        
        # Update progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Finalizing CV', 'progress': 90}
        )
        
        # Step 3: Update CV record with results
        generation_time = time.time() - start_time
        
        success = cv_service.update_cv_status(
            cv_id=cv_id,
            status=CVStatus.SUCCESS,
            latex_code=latex_code,
            pdf_path=pdf_path,
            jpg_path=jpg_path,
            generation_time=generation_time
        )
        
        if not success:
            raise Exception("Failed to update CV record in database")
        
        logger.info(f'Successfully generated CV {cv_id} in {generation_time:.2f} seconds')
        
        return {
            'cv_id': cv_id,
            'status': 'success',
            'generation_time': generation_time,
            'has_pdf': bool(pdf_path),
            'has_jpg': bool(jpg_path),
            'message': 'CV generated successfully'
        }
        
    except Exception as e:
        error_message = str(e)
        generation_time = time.time() - start_time
        
        logger.error(f'CV generation failed for CV {cv_id}: {error_message}')
        
        # Update CV record with error
        cv_service.update_cv_status(
            cv_id=cv_id,
            status=CVStatus.FAILED,
            error_message=error_message,
            generation_time=generation_time
        )
        
        # Update task state
        self.update_state(
            state='FAILURE',
            meta={
                'error': error_message,
                'cv_id': cv_id,
                'generation_time': generation_time
            }
        )
        
        raise Exception(error_message)


@celery.task(bind=True, name='edit_cv_task')
def edit_cv_task(self, cv_id, edit_instructions, user_tier):
    """
    Celery task for editing existing CV using Gemini and LaTeX.
    
    Args:
        cv_id (int): CV database ID
        edit_instructions (str): Instructions for editing the CV
        user_tier (str): User subscription tier
    """
    cv_service = CVService()
    start_time = time.time()
    
    try:
        logger.info(f'Starting CV editing for CV {cv_id}')
        
        # Get existing CV
        from app.models import CV
        cv = CV.query.get(cv_id)
        if not cv:
            raise Exception(f"CV {cv_id} not found in database")
        
        # Update task progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Analyzing edit instructions', 'progress': 20}
        )
        
        # Step 1: Edit LaTeX code using Gemini
        if not cv.latex_code:
            raise Exception("No existing LaTeX code found for editing")
        
        # Parse user data back to dict
        try:
            user_data = json.loads(cv.user_data) if isinstance(cv.user_data, str) else cv.user_data
        except json.JSONDecodeError:
            raise Exception("Invalid user data format in CV record")
        
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Editing CV content with AI', 'progress': 40}
        )
        
        edited_latex_code = edit_cv_with_gemini(
            cv.latex_code, 
            edit_instructions,
            user_data,
            cv.job_description
        )
        
        if not edited_latex_code:
            raise Exception("Failed to edit LaTeX code with Gemini")
        
        logger.info(f'Edited LaTeX code for CV {cv_id}')
        
        # Update progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Compiling updated PDF', 'progress': 70}
        )
        
        # Step 2: Compile edited LaTeX to PDF
        pdf_path, jpg_path = compile_latex_to_pdf(cv.uuid, edited_latex_code, user_tier)
        
        logger.info(f'Compiled edited PDF for CV {cv_id}')
        
        # Update progress
        self.update_state(
            state='PROGRESS',
            meta={'step': 'Finalizing edited CV', 'progress': 90}
        )
        
        # Step 3: Update CV record with results
        generation_time = time.time() - start_time
        
        success = cv_service.update_cv_status(
            cv_id=cv_id,
            status=CVStatus.SUCCESS,
            latex_code=edited_latex_code,
            pdf_path=pdf_path,
            jpg_path=jpg_path,
            generation_time=generation_time
        )
        
        if not success:
            raise Exception("Failed to update CV record in database")
        
        logger.info(f'Successfully edited CV {cv_id} in {generation_time:.2f} seconds')
        
        return {
            'cv_id': cv_id,
            'status': 'success',
            'generation_time': generation_time,
            'has_pdf': bool(pdf_path),
            'has_jpg': bool(jpg_path),
            'message': 'CV edited successfully'
        }
        
    except Exception as e:
        error_message = str(e)
        generation_time = time.time() - start_time
        
        logger.error(f'CV editing failed for CV {cv_id}: {error_message}')
        
        # Update CV record with error
        cv_service.update_cv_status(
            cv_id=cv_id,
            status=CVStatus.FAILED,
            error_message=error_message,
            generation_time=generation_time
        )
        
        # Update task state
        self.update_state(
            state='FAILURE',
            meta={
                'error': error_message,
                'cv_id': cv_id,
                'generation_time': generation_time
            }
        )
        
        raise Exception(error_message)


@celery.task(name='cleanup_task')
def cleanup_task():
    """
    Periodic cleanup task for maintenance.
    """
    try:
        logger.info('Starting cleanup task')
        
        # Cleanup expired tokens
        from app.services.auth_service import AuthService
        auth_service = AuthService()
        token_cleanup_result = auth_service.cleanup_expired_tokens()
        
        # Cleanup orphaned files
        cv_service = CVService()
        file_cleanup_result = cv_service.cleanup_orphaned_files()
        
        result = {
            'status': 'success',
            'token_cleanup': token_cleanup_result,
            'file_cleanup': file_cleanup_result,
            'timestamp': time.time()
        }
        
        logger.info(f'Cleanup task completed: {result}')
        return result
        
    except Exception as e:
        error_message = str(e)
        logger.error(f'Cleanup task failed: {error_message}')
        
        return {
            'status': 'failed',
            'error': error_message,
            'timestamp': time.time()
        }


@celery.task(name='health_check_task')
def health_check_task():
    """
    Health check task for monitoring Celery workers.
    """
    try:
        # Basic health checks
        from app.models import db, User
        
        # Check database connection
        try:
            db.session.execute(text('SELECT 1'))
            db_status = 'healthy'
        except Exception as e:
            db_status = f'unhealthy: {str(e)}'
        
        # Check if we can access models
        try:
            user_count = User.query.count()
            model_status = 'healthy'
        except Exception as e:
            model_status = f'unhealthy: {str(e)}'
            user_count = None
        
        result = {
            'status': 'healthy' if db_status == 'healthy' and model_status == 'healthy' else 'unhealthy',
            'database': db_status,
            'models': model_status,
            'user_count': user_count,
            'timestamp': time.time(),
            'worker_id': current_task.request.hostname if current_task else 'unknown'
        }
        
        logger.info(f'Health check completed: {result["status"]}')
        return result
        
    except Exception as e:
        error_message = str(e)
        logger.error(f'Health check failed: {error_message}')
        
        return {
            'status': 'unhealthy',
            'error': error_message,
            'timestamp': time.time(),
            'worker_id': current_task.request.hostname if current_task else 'unknown'
        }


@celery.task(name='batch_cv_generation_task')
def batch_cv_generation_task(cv_data_list):
    """
    Task for batch CV generation (enterprise feature).
    
    Args:
        cv_data_list (list): List of CV data dictionaries
    """
    try:
        results = []
        cv_service = CVService()
        
        for i, cv_data in enumerate(cv_data_list):
            try:
                # Update progress
                progress = int((i / len(cv_data_list)) * 100)
                current_task.update_state(
                    state='PROGRESS',
                    meta={'step': f'Processing CV {i+1}/{len(cv_data_list)}', 'progress': progress}
                )
                
                # Generate individual CV
                result = generate_cv_task.delay(
                    cv_data['cv_id'],
                    cv_data['user_data'],
                    cv_data['job_description'],
                    cv_data['template_name'],
                    cv_data['user_tier']
                ).get()  # Wait for completion
                
                results.append({
                    'cv_id': cv_data['cv_id'],
                    'status': 'success',
                    'result': result
                })
                
            except Exception as e:
                results.append({
                    'cv_id': cv_data.get('cv_id', 'unknown'),
                    'status': 'failed',
                    'error': str(e)
                })
        
        successful_count = len([r for r in results if r['status'] == 'success'])
        failed_count = len(results) - successful_count
        
        logger.info(f'Batch CV generation completed: {successful_count} successful, {failed_count} failed')
        
        return {
            'status': 'completed',
            'total_cvs': len(cv_data_list),
            'successful_count': successful_count,
            'failed_count': failed_count,
            'results': results
        }
        
    except Exception as e:
        error_message = str(e)
        logger.error(f'Batch CV generation failed: {error_message}')
        
        current_task.update_state(
            state='FAILURE',
            meta={'error': error_message}
        )
        
        raise Exception(error_message)


# Celery beat schedule for periodic tasks
from celery.schedules import crontab

celery.conf.beat_schedule = {
    'cleanup-every-hour': {
        'task': 'cleanup_task',
        'schedule': crontab(minute=0),  # Run every hour
    },
    'health-check-every-5-minutes': {
        'task': 'health_check_task',
        'schedule': crontab(minute='*/5'),  # Run every 5 minutes
    },
}

celery.conf.timezone = 'UTC'

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\utils\decorators.py -----
from functools import wraps
from flask import request, jsonify, current_app
from app.services.auth_service import AuthService
from app.models import User, UserTier
import logging

logger = logging.getLogger(__name__)


def jwt_required(f):
    """
    Decorator to require valid JWT token for route access.
    
    Adds the following to the request object:
    - request.current_user: User object
    - request.token_claims: JWT payload
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Get token from Authorization header
        auth_header = request.headers.get('Authorization')
        
        if not auth_header:
            return jsonify({
                'error': 'Missing Authorization header',
                'message': 'Authorization header is required'
            }), 401
        
        # Extract token from "Bearer <token>" format
        try:
            token_type, token = auth_header.split(' ', 1)
            if token_type.lower() != 'bearer':
                return jsonify({
                    'error': 'Invalid token type',
                    'message': 'Authorization header must be in format: Bearer <token>'
                }), 401
        except ValueError:
            return jsonify({
                'error': 'Invalid Authorization header format',
                'message': 'Authorization header must be in format: Bearer <token>'
            }), 401
        
        # Validate token
        auth_service = AuthService()
        user, payload = auth_service.validate_user_token(token, 'access')
        
        if not user or not payload:
            return jsonify({
                'error': 'Invalid or expired token',
                'message': 'Please login again'
            }), 401
        
        # Add user and token info to request
        request.current_user = user
        request.token_claims = payload
        
        return f(*args, **kwargs)
    
    return decorated_function


def subscription_required(tier=UserTier.PRO):
    """
    Decorator to require specific subscription tier.
    
    Args:
        tier (UserTier): Minimum required subscription tier
    """
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            # This decorator should be used after jwt_required
            if not hasattr(request, 'current_user'):
                return jsonify({
                    'error': 'Authentication required',
                    'message': 'This decorator requires jwt_required to be applied first'
                }), 500
            
            user = request.current_user
            
            # Define tier hierarchy
            tier_levels = {
                UserTier.FREE: 0,
                UserTier.PRO: 1,
                UserTier.ENTERPRISE: 2
            }
            
            user_level = tier_levels.get(user.user_tier, 0)
            required_level = tier_levels.get(tier, 1)
            
            if user_level < required_level:
                return jsonify({
                    'error': 'Subscription required',
                    'message': f'This feature requires {tier.value} subscription',
                    'current_tier': user.user_tier.value,
                    'required_tier': tier.value
                }), 403
            
            return f(*args, **kwargs)
        
        return decorated_function
    return decorator


def validate_json(f):
    """
    Decorator to validate that request contains valid JSON.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not request.is_json:
            return jsonify({
                'error': 'Invalid content type',
                'message': 'Request must contain valid JSON'
            }), 400
        
        try:
            request.get_json()
        except Exception:
            return jsonify({
                'error': 'Invalid JSON',
                'message': 'Request body contains invalid JSON'
            }), 400
        
        return f(*args, **kwargs)
    
    return decorated_function


def generation_limit_check(f):
    """
    Decorator to check if user can generate CVs based on their tier and remaining generations.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # This decorator should be used after jwt_required
        if not hasattr(request, 'current_user'):
            return jsonify({
                'error': 'Authentication required',
                'message': 'This decorator requires jwt_required to be applied first'
            }), 500
        
        user = request.current_user
        
        if not user.can_generate_cv():
            return jsonify({
                'error': 'Generation limit exceeded',
                'message': 'You have reached your CV generation limit',
                'generations_left': user.generations_left,
                'user_tier': user.user_tier.value,
                'upgrade_required': user.user_tier == UserTier.FREE
            }), 403
        
        return f(*args, **kwargs)
    
    return decorated_function


def admin_required(f):
    """
    Decorator to require admin privileges.
    Note: This is a placeholder - you would need to add admin fields to User model.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # This decorator should be used after jwt_required
        if not hasattr(request, 'current_user'):
            return jsonify({
                'error': 'Authentication required',
                'message': 'This decorator requires jwt_required to be applied first'
            }), 500
        
        user = request.current_user
        
        # For now, check if user has enterprise tier as admin proxy
        # In production, you'd add an is_admin field to User model
        if user.user_tier != UserTier.ENTERPRISE:
            return jsonify({
                'error': 'Admin access required',
                'message': 'This endpoint requires administrator privileges'
            }), 403
        
        return f(*args, **kwargs)
    
    return decorated_function


def rate_limit_handler(f):
    """
    Decorator to handle rate limiting errors with custom responses.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except Exception as e:
            if 'rate limit' in str(e).lower():
                return jsonify({
                    'error': 'Rate limit exceeded',
                    'message': 'Too many requests. Please try again later.',
                    'retry_after': getattr(e, 'retry_after', 60)
                }), 429
            raise e
    
    return decorated_function


def log_request(f):
    """
    Decorator to log API requests for monitoring and debugging.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Log request info
        user_id = None
        if hasattr(request, 'current_user'):
            user_id = request.current_user.id
        
        logger.info(
            f'API Request: {request.method} {request.path} '
            f'- User: {user_id} - IP: {request.remote_addr}'
        )
        
        try:
            response = f(*args, **kwargs)
            
            # Log response status if it's a tuple
            if isinstance(response, tuple):
                status_code = response[1]
                logger.info(f'API Response: {status_code} for {request.path}')
            
            return response
            
        except Exception as e:
            logger.error(f'API Error: {str(e)} for {request.path}')
            raise e
    
    return decorated_function


def cors_headers(f):
    """
    Decorator to add CORS headers to response.
    Note: This is mainly for development - use Flask-CORS in production.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        response = f(*args, **kwargs)
        
        # If response is a tuple, get the response object
        if isinstance(response, tuple):
            resp_obj, status_code = response
            if hasattr(resp_obj, 'headers'):
                response_obj = resp_obj
            else:
                # Create response object
                from flask import make_response
                response_obj = make_response(resp_obj, status_code)
        else:
            response_obj = response
        
        # Add CORS headers
        if hasattr(response_obj, 'headers'):
            response_obj.headers['Access-Control-Allow-Origin'] = '*'
            response_obj.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS'
            response_obj.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization'
        
        return response_obj
    
    return decorated_function

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\utils\validators.py -----
from marshmallow import Schema, fields, validate, validates_schema, ValidationError
import re


class LoginSchema(Schema):
    """Schema for login requests."""
    token = fields.Str(required=True, validate=validate.Length(min=1))
    user_info = fields.Dict(required=True)
    
    @validates_schema
    def validate_user_info(self, data, **kwargs):
        user_info = data.get('user_info', {})
        required_fields = ['email', 'name', 'sub']
        
        for field in required_fields:
            if not user_info.get(field):
                raise ValidationError(f'Missing required field in user_info: {field}')
        
        # Validate email format
        email = user_info.get('email')
        if email and not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
            raise ValidationError('Invalid email format')


class TokenRefreshSchema(Schema):
    """Schema for token refresh requests."""
    refresh_token = fields.Str(required=True, validate=validate.Length(min=1))


class CVCreateSchema(Schema):
    """Schema for CV creation requests."""
    title = fields.Str(validate=validate.Length(min=1, max=200), missing='My CV')
    template_name = fields.Str(required=True, validate=validate.OneOf([
        'template_1', 'template_2', 'template_3', 'template_4'
    ]))
    user_data = fields.Dict(required=True)
    job_description = fields.Str(required=True, validate=validate.Length(min=10, max=5000))
    
    @validates_schema
    def validate_user_data(self, data, **kwargs):
        user_data = data.get('user_data', {})
        required_fields = ['name', 'email', 'experience', 'skills']
        
        for field in required_fields:
            if not user_data.get(field):
                raise ValidationError(f'Missing required field in user_data: {field}')
        
        # Validate email in user data
        email = user_data.get('email')
        if email and not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
            raise ValidationError('Invalid email format in user_data')


class CVUpdateSchema(Schema):
    """Schema for CV update requests."""
    title = fields.Str(validate=validate.Length(min=1, max=200))
    user_data = fields.Dict()
    job_description = fields.Str(validate=validate.Length(min=10, max=5000))
    edit_instructions = fields.Str(validate=validate.Length(min=1, max=1000))
    
    @validates_schema
    def validate_update_data(self, data, **kwargs):
        # At least one field should be provided for update
        if not any([
            data.get('title'),
            data.get('user_data'),
            data.get('job_description'),
            data.get('edit_instructions')
        ]):
            raise ValidationError('At least one field must be provided for update')
        
        # If user_data is provided, validate email
        user_data = data.get('user_data')
        if user_data and user_data.get('email'):
            email = user_data['email']
            if not re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', email):
                raise ValidationError('Invalid email format in user_data')


class SubscriptionCreateSchema(Schema):
    """Schema for subscription creation requests."""
    price_id = fields.Str(required=True, validate=validate.Length(min=1))
    success_url = fields.Url()
    cancel_url = fields.Url()


class UserUpdateSchema(Schema):
    """Schema for user profile update requests."""
    name = fields.Str(validate=validate.Length(min=1, max=100))
    email = fields.Email()
    
    @validates_schema
    def validate_update_fields(self, data, **kwargs):
        # At least one field should be provided for update
        if not any([data.get('name'), data.get('email')]):
            raise ValidationError('At least one field must be provided for update')


class PaginationSchema(Schema):
    """Schema for pagination parameters."""
    page = fields.Int(validate=validate.Range(min=1), missing=1)
    per_page = fields.Int(validate=validate.Range(min=1, max=100), missing=10)
    sort_by = fields.Str(validate=validate.OneOf([
        'created_at', 'updated_at', 'title', 'status'
    ]), missing='created_at')
    sort_order = fields.Str(validate=validate.OneOf(['asc', 'desc']), missing='desc')


class CVFilterSchema(PaginationSchema):
    """Schema for CV listing with filters."""
    status = fields.Str(validate=validate.OneOf([
        'pending', 'processing', 'success', 'failed'
    ]))
    template_name = fields.Str(validate=validate.OneOf([
        'template_1', 'template_2', 'template_3', 'template_4'
    ]))
    search = fields.Str(validate=validate.Length(max=100))


class FileUploadSchema(Schema):
    """Schema for file upload validation."""
    file_type = fields.Str(validate=validate.OneOf(['pdf', 'jpg', 'png']))
    max_size = fields.Int(validate=validate.Range(min=1))


class WebhookSchema(Schema):
    """Schema for Stripe webhook validation."""
    type = fields.Str(required=True)
    data = fields.Dict(required=True)
    id = fields.Str(required=True)
    created = fields.Int(required=True)


class PasswordResetSchema(Schema):
    """Schema for password reset requests."""
    email = fields.Email(required=True)


class PasswordChangeSchema(Schema):
    """Schema for password change requests."""
    current_password = fields.Str(required=True, validate=validate.Length(min=8))
    new_password = fields.Str(required=True, validate=validate.Length(min=8))
    confirm_password = fields.Str(required=True)
    
    @validates_schema
    def validate_passwords(self, data, **kwargs):
        if data.get('new_password') != data.get('confirm_password'):
            raise ValidationError('New password and confirmation do not match')
        
        if data.get('current_password') == data.get('new_password'):
            raise ValidationError('New password must be different from current password')


class ContactSchema(Schema):
    """Schema for contact form submissions."""
    name = fields.Str(required=True, validate=validate.Length(min=1, max=100))
    email = fields.Email(required=True)
    subject = fields.Str(required=True, validate=validate.Length(min=1, max=200))
    message = fields.Str(required=True, validate=validate.Length(min=10, max=1000))


class FeedbackSchema(Schema):
    """Schema for user feedback submissions."""
    rating = fields.Int(required=True, validate=validate.Range(min=1, max=5))
    comment = fields.Str(validate=validate.Length(max=1000))
    category = fields.Str(validate=validate.OneOf([
        'general', 'bug_report', 'feature_request', 'performance', 'ui_ux'
    ]), missing='general')


class BulkOperationSchema(Schema):
    """Schema for bulk operations on CVs."""
    cv_ids = fields.List(fields.Int(), required=True, validate=validate.Length(min=1, max=50))
    operation = fields.Str(required=True, validate=validate.OneOf([
        'delete', 'export', 'archive'
    ]))


class SearchSchema(Schema):
    """Schema for search requests."""
    query = fields.Str(required=True, validate=validate.Length(min=1, max=100))
    filters = fields.Dict()
    limit = fields.Int(validate=validate.Range(min=1, max=50), missing=10)


class EmailSchema(Schema):
    """Schema for email validation."""
    email = fields.Email(required=True)


class PhoneSchema(Schema):
    """Schema for phone number validation."""
    phone = fields.Str(required=True, validate=validate.Regexp(
        r'^\+?1?\d{9,15}$',
        error='Invalid phone number format'
    ))


class DateRangeSchema(Schema):
    """Schema for date range queries."""
    start_date = fields.DateTime()
    end_date = fields.DateTime()
    
    @validates_schema
    def validate_date_range(self, data, **kwargs):
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        
        if start_date and end_date and start_date >= end_date:
            raise ValidationError('start_date must be before end_date')


# Custom validation functions
def validate_template_name(template_name):
    """Validate template name format."""
    valid_templates = ['template_1', 'template_2', 'template_3', 'template_4']
    if template_name not in valid_templates:
        raise ValidationError(f'Invalid template name. Must be one of: {", ".join(valid_templates)}')


def validate_cv_uuid(cv_uuid):
    """Validate CV UUID format."""
    import uuid
    try:
        uuid.UUID(cv_uuid)
    except ValueError:
        raise ValidationError('Invalid CV UUID format')


def validate_file_extension(filename, allowed_extensions):
    """Validate file extension."""
    if '.' not in filename:
        raise ValidationError('File must have an extension')
    
    ext = filename.rsplit('.', 1)[1].lower()
    if ext not in allowed_extensions:
        raise ValidationError(f'File extension must be one of: {", ".join(allowed_extensions)}')


def validate_image_file(file_data):
    """Validate image file data."""
    try:
        from PIL import Image
        import io
        
        # Try to open the image
        image = Image.open(io.BytesIO(file_data))
        image.verify()
        
        # Check dimensions
        if image.size[0] > 4096 or image.size[1] > 4096:
            raise ValidationError('Image dimensions too large (max 4096x4096)')
        
    except Exception:
        raise ValidationError('Invalid image file')


def validate_json_structure(data, required_keys):
    """Validate JSON structure has required keys."""
    missing_keys = [key for key in required_keys if key not in data]
    if missing_keys:
        raise ValidationError(f'Missing required keys: {", ".join(missing_keys)}')


# Error formatting helper
def format_validation_errors(errors):
    """Format marshmallow validation errors for API response."""
    formatted_errors = {}
    
    for field, messages in errors.items():
        if isinstance(messages, list):
            formatted_errors[field] = messages[0]  # Take first error message
        else:
            formatted_errors[field] = str(messages)
    
    return {
        'error': 'Validation failed',
        'message': 'Please check your input and try again',
        'validation_errors': formatted_errors
    }

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\config.py -----
import os
from datetime import timedelta
from dotenv import load_dotenv

load_dotenv()

class Config:
    """Base configuration class."""
    
    # Flask Core
    SECRET_KEY = os.environ.get('SECRET_KEY') or 'dev-secret-key-change-in-production'
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    
    # Database
    SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URL') or 'sqlite:///morphcv.db'
    
    # Google OAuth
    GOOGLE_CLIENT_ID = os.environ.get('GOOGLE_CLIENT_ID')
    GOOGLE_CLIENT_SECRET = os.environ.get('GOOGLE_CLIENT_SECRET')
    
    # JWT Configuration
    JWT_SECRET_KEY = os.environ.get('JWT_SECRET_KEY') or SECRET_KEY
    JWT_ACCESS_TOKEN_EXPIRES = timedelta(hours=int(os.environ.get('JWT_ACCESS_TOKEN_EXPIRES', 1)))
    JWT_REFRESH_TOKEN_EXPIRES = timedelta(days=int(os.environ.get('JWT_REFRESH_TOKEN_EXPIRES', 30)))
    JWT_ALGORITHM = 'HS256'
    
    # Celery Configuration
    CELERY_BROKER_URL = os.environ.get('CELERY_BROKER_URL', 'redis://localhost:6379/0')
    CELERY_RESULT_BACKEND = os.environ.get('CELERY_RESULT_BACKEND', 'redis://localhost:6379/0')
    CELERY_TASK_SERIALIZER = 'json'
    CELERY_RESULT_SERIALIZER = 'json'
    CELERY_ACCEPT_CONTENT = ['json']
    CELERY_TIMEZONE = 'UTC'
    CELERY_ENABLE_UTC = True
    
    # Stripe Configuration
    STRIPE_PUBLIC_KEY = os.environ.get('STRIPE_PUBLIC_KEY')
    STRIPE_SECRET_KEY = os.environ.get('STRIPE_SECRET_KEY')
    STRIPE_WEBHOOK_SECRET = os.environ.get('STRIPE_WEBHOOK_SECRET')
    
    # AI Services
    GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')
    
    # File Upload Configuration
    MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB max file size
    UPLOAD_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'user_data')
    TEMPLATE_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'latex_templates')
    
    # Rate Limiting
    RATELIMIT_STORAGE_URL = os.environ.get('CELERY_BROKER_URL', 'redis://localhost:6379/1')
    RATELIMIT_DEFAULT = "100 per hour"
    
    # CORS Configuration
    CORS_ORIGINS = ['http://localhost:3000', 'http://localhost:5173']  # React dev servers
    
    # Security Headers
    SECURITY_HEADERS = {
        'X-Content-Type-Options': 'nosniff',
        'X-Frame-Options': 'DENY',
        'X-XSS-Protection': '1; mode=block',
        'Strict-Transport-Security': 'max-age=31536000; includeSubDomains',
        'Content-Security-Policy': "default-src 'self'"
    }


class DevelopmentConfig(Config):
    """Development configuration."""
    DEBUG = True
    CORS_ORIGINS = ['http://localhost:3000', 'http://localhost:5173', 'http://127.0.0.1:3000']
    

class ProductionConfig(Config):
    """Production configuration."""
    DEBUG = False
    
    # Security
    SESSION_COOKIE_SECURE = True
    SESSION_COOKIE_HTTPONLY = True
    SESSION_COOKIE_SAMESITE = 'Lax'
    
    # Performance
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 10,
        'pool_recycle': 120,
        'pool_pre_ping': True
    }
    
    # Update CORS for production domains
    CORS_ORIGINS = os.environ.get('CORS_ORIGINS', '').split(',')


class TestingConfig(Config):
    """Testing configuration."""
    TESTING = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///:memory:'
    WTF_CSRF_ENABLED = False
    JWT_ACCESS_TOKEN_EXPIRES = timedelta(minutes=5)


# Configuration mapping
config = {
    'development': DevelopmentConfig,
    'production': ProductionConfig,
    'testing': TestingConfig,
    'default': DevelopmentConfig
}


def get_config():
    """Get configuration class based on environment."""
    return config[os.environ.get('FLASK_ENV', 'default')]

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\models.py -----
from flask_sqlalchemy import SQLAlchemy
from flask_login import UserMixin
from datetime import datetime, timezone
import uuid
import enum

db = SQLAlchemy()


class UserTier(enum.Enum):
    """User subscription tiers."""
    FREE = "free"
    PRO = "pro"
    ENTERPRISE = "enterprise"


class CVStatus(enum.Enum):
    """CV generation status."""
    PENDING = "pending"
    PROCESSING = "processing"
    SUCCESS = "success"
    FAILED = "failed"


class User(UserMixin, db.Model):
    """User model with subscription and authentication support."""
    __tablename__ = 'users'
    
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(120), unique=True, nullable=False, index=True)
    password_hash = db.Column(db.String(255), nullable=True)
    
    # Google OAuth fields
    google_id = db.Column(db.String(100), unique=True, nullable=True, index=True)
    name = db.Column(db.String(100), nullable=True)
    profile_pic = db.Column(db.String(200), nullable=True)
    
    # Subscription fields
    user_tier = db.Column(db.Enum(UserTier), default=UserTier.FREE, nullable=False)
    generations_left = db.Column(db.Integer, default=2)
    stripe_customer_id = db.Column(db.String(100), nullable=True, index=True)
    subscription_id = db.Column(db.String(100), nullable=True)
    subscription_status = db.Column(db.String(50), nullable=True)
    subscription_current_period_end = db.Column(db.DateTime(timezone=True), nullable=True)
    
    # Timestamps
    created_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    updated_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc), 
                          onupdate=lambda: datetime.now(timezone.utc))
    last_login = db.Column(db.DateTime(timezone=True), nullable=True)
    
    # Relationships
    cvs = db.relationship('CV', backref='user', lazy=True, cascade='all, delete-orphan')
    tokens = db.relationship('TokenBlacklist', backref='user', lazy=True, cascade='all, delete-orphan')
    
    def __repr__(self):
        return f'<User {self.email}>'
    
    def to_dict(self):
        """Convert user to dictionary for API responses."""
        return {
            'id': self.id,
            'email': self.email,
            'name': self.name,
            'profile_pic': self.profile_pic,
            'user_tier': self.user_tier.value,
            'generations_left': self.generations_left,
            'subscription_status': self.subscription_status,
            'subscription_current_period_end': self.subscription_current_period_end.isoformat() if self.subscription_current_period_end else None,
            'created_at': self.created_at.isoformat(),
            'last_login': self.last_login.isoformat() if self.last_login else None
        }
    
    def can_generate_cv(self):
        """Check if user can generate a new CV."""
        if self.user_tier == UserTier.FREE:
            return self.generations_left > 0
        return True  # Pro and Enterprise have unlimited generations
    
    def use_generation(self):
        """Decrement generation count for free users."""
        if self.user_tier == UserTier.FREE and self.generations_left > 0:
            self.generations_left -= 1
            db.session.commit()


class CV(db.Model):
    """CV model with enhanced tracking and file management."""
    __tablename__ = 'cvs'
    
    id = db.Column(db.Integer, primary_key=True)
    uuid = db.Column(db.String(36), unique=True, default=lambda: str(uuid.uuid4()), index=True)
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False, index=True)
    
    # CV metadata
    title = db.Column(db.String(200), nullable=False, default='Untitled CV')
    template_name = db.Column(db.String(50), nullable=False)
    
    # User input data
    user_data = db.Column(db.Text, nullable=False)
    job_description = db.Column(db.Text, nullable=False)
    
    # Generated content
    latex_code = db.Column(db.Text, nullable=True)
    pdf_path = db.Column(db.String(255), nullable=True)
    jpg_path = db.Column(db.String(255), nullable=True)
    
    # Processing status
    status = db.Column(db.Enum(CVStatus), default=CVStatus.PENDING, nullable=False)
    task_id = db.Column(db.String(50), nullable=True, index=True)
    error_message = db.Column(db.Text, nullable=True)
    
    # File metadata
    pdf_size = db.Column(db.Integer, nullable=True)  # File size in bytes
    generation_time = db.Column(db.Float, nullable=True)  # Processing time in seconds
    
    # Timestamps
    created_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    updated_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc),
                          onupdate=lambda: datetime.now(timezone.utc))
    last_downloaded = db.Column(db.DateTime(timezone=True), nullable=True)
    
    def __repr__(self):
        return f'<CV {self.uuid}>'
    
    def to_dict(self, include_sensitive=False):
        """Convert CV to dictionary for API responses."""
        data = {
            'id': self.id,
            'uuid': self.uuid,
            'title': self.title,
            'template_name': self.template_name,
            'status': self.status.value,
            'error_message': self.error_message,
            'created_at': self.created_at.isoformat(),
            'updated_at': self.updated_at.isoformat(),
            'last_downloaded': self.last_downloaded.isoformat() if self.last_downloaded else None,
            'has_pdf': bool(self.pdf_path),
            'has_jpg': bool(self.jpg_path),
            'pdf_size': self.pdf_size,
            'generation_time': self.generation_time
        }
        
        if include_sensitive:
            data.update({
                'user_data': self.user_data,
                'job_description': self.job_description,
                'latex_code': self.latex_code
            })
        
        return data


class TokenBlacklist(db.Model):
    """Blacklisted JWT tokens for logout functionality."""
    __tablename__ = 'token_blacklist'
    
    id = db.Column(db.Integer, primary_key=True)
    jti = db.Column(db.String(36), nullable=False, unique=True, index=True)  # JWT ID
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False, index=True)
    token_type = db.Column(db.String(20), nullable=False)  # 'access' or 'refresh'
    revoked_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    expires_at = db.Column(db.DateTime(timezone=True), nullable=False)
    
    def __repr__(self):
        return f'<TokenBlacklist {self.jti}>'


class DownloadToken(db.Model):
    """Temporary tokens for secure file downloads."""
    __tablename__ = 'download_tokens'
    
    id = db.Column(db.Integer, primary_key=True)
    token = db.Column(db.String(36), unique=True, default=lambda: str(uuid.uuid4()), index=True)
    cv_id = db.Column(db.Integer, db.ForeignKey('cvs.id'), nullable=False)
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'), nullable=False)
    file_type = db.Column(db.String(10), nullable=False)  # 'pdf' or 'jpg'
    expires_at = db.Column(db.DateTime(timezone=True), nullable=False)
    used = db.Column(db.Boolean, default=False)
    created_at = db.Column(db.DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    
    # Relationships
    cv = db.relationship('CV', backref='download_tokens')
    user = db.relationship('User', backref='download_tokens')
    
    def __repr__(self):
        return f'<DownloadToken {self.token}>'
    
    def is_valid(self):
        """Check if download token is still valid."""
        now = datetime.now(timezone.utc)
        return not self.used and self.expires_at > now


# Create indexes for better query performance
db.Index('idx_user_email', User.email)
db.Index('idx_user_google_id', User.google_id)
db.Index('idx_user_stripe_customer', User.stripe_customer_id)
db.Index('idx_cv_user_id', CV.user_id)
db.Index('idx_cv_uuid', CV.uuid)
db.Index('idx_cv_status', CV.status)
db.Index('idx_cv_task_id', CV.task_id)
db.Index('idx_token_blacklist_jti', TokenBlacklist.jti)
db.Index('idx_token_blacklist_user', TokenBlacklist.user_id)
db.Index('idx_download_token', DownloadToken.token)

----- FILE: E:\$$latex_project\morphcv\flask_backend\app\__init__.py -----
from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from flask_login import LoginManager
from flask_cors import CORS
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from celery import Celery
import os
import logging
from datetime import datetime, timezone

# Import extensions
from app.models import db, User
from app.config import get_config

# Initialize extensions
migrate = Migrate()
login_manager = LoginManager()
cors = CORS()
limiter = Limiter(
    key_func=get_remote_address,
    default_limits=["1000 per hour"]
)

# Initialize Celery
celery = Celery('morphcv')


def create_app(config_name=None):
    """Application factory pattern."""
    app = Flask(__name__)
    
    # Load configuration
    if config_name is None:
        config_name = os.environ.get('FLASK_ENV', 'development')
    
    config_class = get_config()
    app.config.from_object(config_class)
    
    # Initialize extensions with app
    db.init_app(app)
    migrate.init_app(app, db)
    login_manager.init_app(app)
    cors.init_app(app, origins=app.config['CORS_ORIGINS'], supports_credentials=True)
    limiter.init_app(app)
    
    # Configure Celery
    configure_celery(app, celery)
    
    # Configure logging
    configure_logging(app)
    
    # Configure login manager
    login_manager.login_view = 'api.auth.login'
    login_manager.login_message = 'Please log in to access this page.'
    
    @login_manager.user_loader
    def load_user(user_id):
        return User.query.get(int(user_id))
    
    # Register blueprints
    register_blueprints(app)
    
    # Register error handlers
    register_error_handlers(app)
    
    # Add security headers
    @app.after_request
    def add_security_headers(response):
        """Add security headers to all responses."""
        for header, value in app.config.get('SECURITY_HEADERS', {}).items():
            response.headers[header] = value
        return response
    
    # Health check endpoint
    @app.route('/health')
    def health_check():
        """Health check endpoint for monitoring."""
        try:
            # Check database connection
            db.session.execute('SELECT 1')
            db_status = 'healthy'
        except Exception as e:
            db_status = f'unhealthy: {str(e)}'
        
        return jsonify({
            'status': 'healthy' if db_status == 'healthy' else 'unhealthy',
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'database': db_status,
            'version': '1.0.0'
        })
    
    return app


def configure_celery(app, celery):
    """Configure Celery with Flask app context."""
    
    # Update configuration
    celery.conf.update(
        broker_url=app.config['CELERY_BROKER_URL'],
        result_backend=app.config['CELERY_RESULT_BACKEND'],
        task_serializer=app.config.get('CELERY_TASK_SERIALIZER', 'json'),
        result_serializer=app.config.get('CELERY_RESULT_SERIALIZER', 'json'),
        accept_content=app.config.get('CELERY_ACCEPT_CONTENT', ['json']),
        timezone=app.config.get('CELERY_TIMEZONE', 'UTC'),
        enable_utc=app.config.get('CELERY_ENABLE_UTC', True),
        task_track_started=True,
        task_send_sent_event=True,
        worker_send_task_events=True,
        result_expires=3600,  # 1 hour
    )
    
    # Create task base class with app context
    class ContextTask(celery.Task):
        """Make celery tasks work with Flask app context."""
        def __call__(self, *args, **kwargs):
            with app.app_context():
                return self.run(*args, **kwargs)
    
    celery.Task = ContextTask


def register_blueprints(app):
    """Register all application blueprints."""
    
    # API v1 blueprints
    from app.api import api_bp
    app.register_blueprint(api_bp, url_prefix='/api/v1')
    
    # Main routes (for serving React frontend in production)
    from app.main import main_bp
    app.register_blueprint(main_bp)


def register_error_handlers(app):
    """Register global error handlers."""
    
    @app.errorhandler(400)
    def bad_request(error):
        return jsonify({
            'error': 'Bad Request',
            'message': 'The request could not be understood by the server.',
            'status_code': 400
        }), 400
    
    @app.errorhandler(401)
    def unauthorized(error):
        return jsonify({
            'error': 'Unauthorized',
            'message': 'Authentication is required to access this resource.',
            'status_code': 401
        }), 401
    
    @app.errorhandler(403)
    def forbidden(error):
        return jsonify({
            'error': 'Forbidden',
            'message': 'You do not have permission to access this resource.',
            'status_code': 403
        }), 403
    
    @app.errorhandler(404)
    def not_found(error):
        return jsonify({
            'error': 'Not Found',
            'message': 'The requested resource could not be found.',
            'status_code': 404
        }), 404
    
    @app.errorhandler(429)
    def rate_limit_exceeded(error):
        return jsonify({
            'error': 'Rate Limit Exceeded',
            'message': 'Too many requests. Please try again later.',
            'status_code': 429,
            'retry_after': error.retry_after
        }), 429
    
    @app.errorhandler(500)
    def internal_error(error):
        app.logger.error(f'Internal server error: {str(error)}')
        return jsonify({
            'error': 'Internal Server Error',
            'message': 'An unexpected error occurred.',
            'status_code': 500
        }), 500
    
    @app.errorhandler(Exception)
    def handle_exception(error):
        """Handle unexpected exceptions."""
        app.logger.error(f'Unhandled exception: {str(error)}', exc_info=True)
        
        if app.debug:
            # In debug mode, return the actual error
            return jsonify({
                'error': 'Exception',
                'message': str(error),
                'type': type(error).__name__,
                'status_code': 500
            }), 500
        else:
            # In production, return generic error
            return jsonify({
                'error': 'Internal Server Error',
                'message': 'An unexpected error occurred.',
                'status_code': 500
            }), 500


def configure_logging(app):
    """Configure application logging."""
    
    if not app.debug and not app.testing:
        # Production logging configuration
        if not os.path.exists('logs'):
            os.mkdir('logs')
        
        file_handler = logging.FileHandler('logs/morphcv.log')
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
        ))
        file_handler.setLevel(logging.INFO)
        app.logger.addHandler(file_handler)
        
        app.logger.setLevel(logging.INFO)
        app.logger.info('MorphCV startup')


# Make celery available for imports
__all__ = ['create_app', 'celery']

----- FILE: E:\$$latex_project\morphcv\flask_backend\migrations\versions\0708ff47c2d5_.py -----
"""empty message

Revision ID: 0708ff47c2d5
Revises: 
Create Date: 2025-06-17 15:30:03.010823

"""
from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision = '0708ff47c2d5'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('users',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('email', sa.String(length=120), nullable=False),
    sa.Column('password_hash', sa.String(length=255), nullable=True),
    sa.Column('google_id', sa.String(length=100), nullable=True),
    sa.Column('name', sa.String(length=100), nullable=True),
    sa.Column('profile_pic', sa.String(length=200), nullable=True),
    sa.Column('user_tier', sa.Enum('FREE', 'PRO', 'ENTERPRISE', name='usertier'), nullable=False),
    sa.Column('generations_left', sa.Integer(), nullable=True),
    sa.Column('stripe_customer_id', sa.String(length=100), nullable=True),
    sa.Column('subscription_id', sa.String(length=100), nullable=True),
    sa.Column('subscription_status', sa.String(length=50), nullable=True),
    sa.Column('subscription_current_period_end', sa.DateTime(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.Column('last_login', sa.DateTime(), nullable=True),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('users', schema=None) as batch_op:
        batch_op.create_index('idx_user_email', ['email'], unique=False)
        batch_op.create_index('idx_user_google_id', ['google_id'], unique=False)
        batch_op.create_index('idx_user_stripe_customer', ['stripe_customer_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_users_email'), ['email'], unique=True)
        batch_op.create_index(batch_op.f('ix_users_google_id'), ['google_id'], unique=True)
        batch_op.create_index(batch_op.f('ix_users_stripe_customer_id'), ['stripe_customer_id'], unique=False)

    op.create_table('cvs',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('uuid', sa.String(length=36), nullable=True),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('title', sa.String(length=200), nullable=False),
    sa.Column('template_name', sa.String(length=50), nullable=False),
    sa.Column('user_data', sa.Text(), nullable=False),
    sa.Column('job_description', sa.Text(), nullable=False),
    sa.Column('latex_code', sa.Text(), nullable=True),
    sa.Column('pdf_path', sa.String(length=255), nullable=True),
    sa.Column('jpg_path', sa.String(length=255), nullable=True),
    sa.Column('status', sa.Enum('PENDING', 'PROCESSING', 'SUCCESS', 'FAILED', name='cvstatus'), nullable=False),
    sa.Column('task_id', sa.String(length=50), nullable=True),
    sa.Column('error_message', sa.Text(), nullable=True),
    sa.Column('pdf_size', sa.Integer(), nullable=True),
    sa.Column('generation_time', sa.Float(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.Column('updated_at', sa.DateTime(), nullable=True),
    sa.Column('last_downloaded', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('cvs', schema=None) as batch_op:
        batch_op.create_index('idx_cv_status', ['status'], unique=False)
        batch_op.create_index('idx_cv_task_id', ['task_id'], unique=False)
        batch_op.create_index('idx_cv_user_id', ['user_id'], unique=False)
        batch_op.create_index('idx_cv_uuid', ['uuid'], unique=False)
        batch_op.create_index(batch_op.f('ix_cvs_task_id'), ['task_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_cvs_user_id'), ['user_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_cvs_uuid'), ['uuid'], unique=True)

    op.create_table('token_blacklist',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('jti', sa.String(length=36), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('token_type', sa.String(length=20), nullable=False),
    sa.Column('revoked_at', sa.DateTime(), nullable=True),
    sa.Column('expires_at', sa.DateTime(), nullable=False),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('token_blacklist', schema=None) as batch_op:
        batch_op.create_index('idx_token_blacklist_jti', ['jti'], unique=False)
        batch_op.create_index('idx_token_blacklist_user', ['user_id'], unique=False)
        batch_op.create_index(batch_op.f('ix_token_blacklist_jti'), ['jti'], unique=True)
        batch_op.create_index(batch_op.f('ix_token_blacklist_user_id'), ['user_id'], unique=False)

    op.create_table('download_tokens',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('token', sa.String(length=36), nullable=True),
    sa.Column('cv_id', sa.Integer(), nullable=False),
    sa.Column('user_id', sa.Integer(), nullable=False),
    sa.Column('file_type', sa.String(length=10), nullable=False),
    sa.Column('expires_at', sa.DateTime(), nullable=False),
    sa.Column('used', sa.Boolean(), nullable=True),
    sa.Column('created_at', sa.DateTime(), nullable=True),
    sa.ForeignKeyConstraint(['cv_id'], ['cvs.id'], ),
    sa.ForeignKeyConstraint(['user_id'], ['users.id'], ),
    sa.PrimaryKeyConstraint('id')
    )
    with op.batch_alter_table('download_tokens', schema=None) as batch_op:
        batch_op.create_index('idx_download_token', ['token'], unique=False)
        batch_op.create_index(batch_op.f('ix_download_tokens_token'), ['token'], unique=True)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('download_tokens', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_download_tokens_token'))
        batch_op.drop_index('idx_download_token')

    op.drop_table('download_tokens')
    with op.batch_alter_table('token_blacklist', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_token_blacklist_user_id'))
        batch_op.drop_index(batch_op.f('ix_token_blacklist_jti'))
        batch_op.drop_index('idx_token_blacklist_user')
        batch_op.drop_index('idx_token_blacklist_jti')

    op.drop_table('token_blacklist')
    with op.batch_alter_table('cvs', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_cvs_uuid'))
        batch_op.drop_index(batch_op.f('ix_cvs_user_id'))
        batch_op.drop_index(batch_op.f('ix_cvs_task_id'))
        batch_op.drop_index('idx_cv_uuid')
        batch_op.drop_index('idx_cv_user_id')
        batch_op.drop_index('idx_cv_task_id')
        batch_op.drop_index('idx_cv_status')

    op.drop_table('cvs')
    with op.batch_alter_table('users', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('ix_users_stripe_customer_id'))
        batch_op.drop_index(batch_op.f('ix_users_google_id'))
        batch_op.drop_index(batch_op.f('ix_users_email'))
        batch_op.drop_index('idx_user_stripe_customer')
        batch_op.drop_index('idx_user_google_id')
        batch_op.drop_index('idx_user_email')

    op.drop_table('users')
    # ### end Alembic commands ###

----- FILE: E:\$$latex_project\morphcv\flask_backend\migrations\versions\1f303c3e9a88_convert_datetime_columns_to_use_.py -----
"""convert datetime columns to use timezones

Revision ID: 1f303c3e9a88
Revises: 0708ff47c2d5
Create Date: 2025-06-18 07:26:47.075213

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '1f303c3e9a88'
down_revision = '0708ff47c2d5'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('cvs', schema=None) as batch_op:
        batch_op.alter_column('created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('last_downloaded',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)

    with op.batch_alter_table('download_tokens', schema=None) as batch_op:
        batch_op.alter_column('expires_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)
        batch_op.alter_column('created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)

    with op.batch_alter_table('token_blacklist', schema=None) as batch_op:
        batch_op.alter_column('revoked_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('expires_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=False)

    with op.batch_alter_table('users', schema=None) as batch_op:
        batch_op.alter_column('subscription_current_period_end',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('created_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('updated_at',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)
        batch_op.alter_column('last_login',
               existing_type=postgresql.TIMESTAMP(),
               type_=sa.DateTime(timezone=True),
               existing_nullable=True)

    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('users', schema=None) as batch_op:
        batch_op.alter_column('last_login',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('subscription_current_period_end',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)

    with op.batch_alter_table('token_blacklist', schema=None) as batch_op:
        batch_op.alter_column('expires_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)
        batch_op.alter_column('revoked_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)

    with op.batch_alter_table('download_tokens', schema=None) as batch_op:
        batch_op.alter_column('created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('expires_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=False)

    with op.batch_alter_table('cvs', schema=None) as batch_op:
        batch_op.alter_column('last_downloaded',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('updated_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)
        batch_op.alter_column('created_at',
               existing_type=sa.DateTime(timezone=True),
               type_=postgresql.TIMESTAMP(),
               existing_nullable=True)

    # ### end Alembic commands ###

----- FILE: E:\$$latex_project\morphcv\flask_backend\migrations\env.py -----
import logging
from logging.config import fileConfig

from flask import current_app

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)
logger = logging.getLogger('alembic.env')


def get_engine():
    try:
        # this works with Flask-SQLAlchemy<3 and Alchemical
        return current_app.extensions['migrate'].db.get_engine()
    except (TypeError, AttributeError):
        # this works with Flask-SQLAlchemy>=3
        return current_app.extensions['migrate'].db.engine


def get_engine_url():
    try:
        return get_engine().url.render_as_string(hide_password=False).replace(
            '%', '%%')
    except AttributeError:
        return str(get_engine().url).replace('%', '%%')


# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
config.set_main_option('sqlalchemy.url', get_engine_url())
target_db = current_app.extensions['migrate'].db

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_metadata():
    if hasattr(target_db, 'metadatas'):
        return target_db.metadatas[None]
    return target_db.metadata


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url, target_metadata=get_metadata(), literal_binds=True
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    # this callback is used to prevent an auto-migration from being generated
    # when there are no changes to the schema
    # reference: http://alembic.zzzcomputing.com/en/latest/cookbook.html
    def process_revision_directives(context, revision, directives):
        if getattr(config.cmd_opts, 'autogenerate', False):
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')

    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get("process_revision_directives") is None:
        conf_args["process_revision_directives"] = process_revision_directives

    connectable = get_engine()

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=get_metadata(),
            **conf_args
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

----- FILE: E:\$$latex_project\morphcv\flask_backend\docker-compose.yml -----
version: '3.8'

services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    container_name: morphcv-db
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-morphcv}
      POSTGRES_USER: ${POSTGRES_USER:-morphcv}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    networks:
      - morphcv-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-morphcv}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cache and Message Broker
  redis:
    image: redis:7-alpine
    container_name: morphcv-redis
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "${REDIS_PORT:-6379}:6379"
    networks:
      - morphcv-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Flask Web Application
  web:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: morphcv-web
    environment:
      - FLASK_ENV=${FLASK_ENV:-production}
      - SECRET_KEY=${SECRET_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-morphcv}:${POSTGRES_PASSWORD:-password}@db:5432/${POSTGRES_DB:-morphcv}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - STRIPE_PUBLIC_KEY=${STRIPE_PUBLIC_KEY}
      - STRIPE_SECRET_KEY=${STRIPE_SECRET_KEY}
      - STRIPE_WEBHOOK_SECRET=${STRIPE_WEBHOOK_SECRET}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000,http://localhost:5173}
    volumes:
      - user_data:/app/user_data
      - ./latex_templates:/app/latex_templates:ro
    ports:
      - "${WEB_PORT:-5000}:5000"
    depends_on:
      - db
      - redis
    networks:
      - morphcv-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Worker
  worker:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: morphcv-worker
    command: celery -A run.celery worker --loglevel=info --concurrency=2
    environment:
      - FLASK_ENV=${FLASK_ENV:-production}
      - SECRET_KEY=${SECRET_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-morphcv}:${POSTGRES_PASSWORD:-password}@db:5432/${POSTGRES_DB:-morphcv}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - GEMINI_API_KEY=${GEMINI_API_KEY}
    volumes:
      - user_data:/app/user_data
      - ./latex_templates:/app/latex_templates:ro
    depends_on:
      - db
      - redis
    networks:
      - morphcv-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "celery", "-A", "run.celery", "inspect", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Celery Beat Scheduler
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: morphcv-scheduler
    command: celery -A run.celery beat --loglevel=info --schedule=/app/celerybeat-schedule
    environment:
      - FLASK_ENV=${FLASK_ENV:-production}
      - SECRET_KEY=${SECRET_KEY}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-morphcv}:${POSTGRES_PASSWORD:-password}@db:5432/${POSTGRES_DB:-morphcv}
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    volumes:
      - celery_beat:/app
    depends_on:
      - db
      - redis
    networks:
      - morphcv-network
    restart: unless-stopped

  # Nginx Reverse Proxy (optional)
  nginx:
    image: nginx:alpine
    container_name: morphcv-nginx
    ports:
      - "${NGINX_PORT:-80}:80"
      - "${NGINX_SSL_PORT:-443}:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - user_data:/app/user_data:ro
    depends_on:
      - web
    networks:
      - morphcv-network
    restart: unless-stopped
    profiles:
      - nginx

  # Monitoring with Prometheus (optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: morphcv-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - morphcv-network
    profiles:
      - monitoring

  # Grafana Dashboard (optional)
  grafana:
    image: grafana/grafana:latest
    container_name: morphcv-grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - morphcv-network
    profiles:
      - monitoring

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  user_data:
    driver: local
  celery_beat:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  morphcv-network:
    driver: bridge

# Development override
# Use: docker-compose -f docker-compose.yml -f docker-compose.dev.yml up
---
# docker-compose.dev.yml content for development
version: '3.8'

services:
  web:
    build:
      target: development
    environment:
      - FLASK_ENV=development
    volumes:
      - .:/app
    command: python run.py

  worker:
    build:
      target: development
    environment:
      - FLASK_ENV=development
    volumes:
      - .:/app
    command: celery -A run.celery worker --loglevel=debug
    
  scheduler:
    build:
      target: development
    environment:
      - FLASK_ENV=development
    volumes:
      - .:/app
    # ADDED: This whole section was missing
    command: celery -A run.celery beat --loglevel=info
----- FILE: E:\$$latex_project\morphcv\flask_backend\gunicorn.conf.py -----
"""
Gunicorn configuration file for MorphCV Flask application.

This configuration optimizes the application for production deployment
with proper worker management, logging, and security settings.
"""

import os
import multiprocessing

# Server socket
bind = f"0.0.0.0:{os.environ.get('PORT', '5000')}"
backlog = 2048

# Worker processes
workers = int(os.environ.get('GUNICORN_WORKERS', multiprocessing.cpu_count() * 2 + 1))
worker_class = "sync"
worker_connections = 1000
max_requests = 1000
max_requests_jitter = 50
preload_app = True
timeout = 60
keepalive = 2

# Restart workers after this many requests, to help prevent memory leaks
max_requests = 1000
max_requests_jitter = 50

# Restart workers after this much time has passed
max_worker_lifetime = 12 * 60 * 60  # 12 hours
max_worker_lifetime_jitter = 60 * 60  # 1 hour

# Application
wsgi_module = "run:app"
pythonpath = "."

# Logging
loglevel = os.environ.get('LOG_LEVEL', 'info')
accesslog = "-"  # Log to stdout
errorlog = "-"   # Log to stderr
access_log_format = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s" %(D)s'

# Process naming
proc_name = 'morphcv-flask'

# Security
limit_request_line = 4094
limit_request_fields = 100
limit_request_field_size = 8190

# SSL (if certificates are provided)
keyfile = os.environ.get('SSL_KEYFILE')
certfile = os.environ.get('SSL_CERTFILE')

# Server mechanics
daemon = False
pidfile = '/tmp/gunicorn.pid'
user = os.environ.get('GUNICORN_USER')
group = os.environ.get('GUNICORN_GROUP')
tmp_upload_dir = '/tmp'

# Application callbacks
def when_ready(server):
    """Called just after the server is started."""
    server.log.info("MorphCV Flask server is ready. Listening at: %s", server.address)

def worker_int(worker):
    """Called just after a worker has been killed by a SIGINT signal."""
    worker.log.info("worker received INT or QUIT signal")

def pre_fork(server, worker):
    """Called just before a worker is forked."""
    server.log.info("Worker spawned (pid: %s)", worker.pid)

def post_fork(server, worker):
    """Called just after a worker has been forked."""
    server.log.info("Worker spawned (pid: %s)", worker.pid)

def post_worker_init(worker):
    """Called just after a worker has initialized the application."""
    worker.log.info("Worker initialized (pid: %s)", worker.pid)

def worker_abort(worker):
    """Called when a worker received a SIGABRT signal."""
    worker.log.info("worker received SIGABRT signal")

# Environment-specific configurations
if os.environ.get('FLASK_ENV') == 'development':
    # Development settings
    reload = True
    loglevel = 'debug'
    workers = 1
elif os.environ.get('FLASK_ENV') == 'production':
    # Production settings
    reload = False
    preload_app = True
    
    # Performance optimizations
    worker_tmp_dir = '/dev/shm'  # Use shared memory for better performance
    
    # Security enhancements
    forwarded_allow_ips = '*'
    secure_scheme_headers = {
        'X-FORWARDED-PROTOCOL': 'ssl',
        'X-FORWARDED-PROTO': 'https',
        'X-FORWARDED-SSL': 'on'
    }

# Health check configuration
def on_starting(server):
    """Called just before the master process is initialized."""
    server.log.info("Starting MorphCV Flask server...")

def on_reload(server):
    """Called to recycle workers during a reload via SIGHUP."""
    server.log.info("Reloading MorphCV Flask server...")

# Memory management
def post_request(worker, req, environ, resp):
    """Called just after a worker processes the request."""
    # Optional: Add memory monitoring here
    pass

# Custom error pages
def default_proc_name(name):
    """Return the default process name."""
    return f"morphcv-{name}"

----- FILE: E:\$$latex_project\morphcv\flask_backend\run.py -----
#!/usr/bin/env python3
"""
MorphCV Flask Application Entry Point

This file serves as the main entry point for the MorphCV Flask application.
It can be used for both development and production environments.
"""

import os
import sys
from app import create_app, celery

# Add the current directory to Python path
sys.path.insert(0, os.path.dirname(__file__))

# Create Flask app instance
app = create_app()

def make_celery(app):
    """Create Celery instance with Flask app context."""
    celery.conf.update(app.config)
    
    class ContextTask(celery.Task):
        def __call__(self, *args, **kwargs):
            with app.app_context():
                return self.run(*args, **kwargs)
    
    celery.Task = ContextTask
    return celery

# Configure Celery with Flask app
celery = make_celery(app)

@app.shell_context_processor
def make_shell_context():
    """Provide shell context for flask shell command."""
    from app.models import db, User, CV, TokenBlacklist, DownloadToken
    from app.services.auth_service import AuthService
    from app.services.cv_service import CVService
    from app.services.payment_service import PaymentService
    
    return {
        'db': db,
        'User': User,
        'CV': CV,
        'TokenBlacklist': TokenBlacklist,
        'DownloadToken': DownloadToken,
        'AuthService': AuthService,
        'CVService': CVService,
        'PaymentService': PaymentService,
        'app': app,
        'celery': celery
    }

@app.cli.command()
def init_db():
    """Initialize the database."""
    from app.models import db
    db.create_all()
    print("Database initialized successfully.")

@app.cli.command()
def create_admin():
    """Create an admin user."""
    from app.models import db, User, UserTier
    
    email = input("Enter admin email: ")
    name = input("Enter admin name: ")
    
    # Check if user already exists
    existing_user = User.query.filter_by(email=email).first()
    if existing_user:
        print(f"User with email {email} already exists.")
        return
    
    # Create admin user
    admin_user = User(
        email=email,
        name=name,
        user_tier=UserTier.ENTERPRISE,
        generations_left=999
    )
    
    db.session.add(admin_user)
    db.session.commit()
    
    print(f"Admin user created successfully: {email}")

@app.cli.command()
def cleanup():
    """Run cleanup tasks."""
    from app.services.auth_service import AuthService
    from app.services.cv_service import CVService
    
    auth_service = AuthService()
    cv_service = CVService()
    
    # Cleanup expired tokens
    token_result = auth_service.cleanup_expired_tokens()
    print(f"Cleaned up {token_result.get('cleaned_count', 0)} expired tokens")
    
    # Cleanup orphaned files
    file_result = cv_service.cleanup_orphaned_files()
    print(f"Cleaned up {file_result.get('cleaned_count', 0)} orphaned files")

@app.cli.command()
def test_services():
    """Test all services for basic functionality."""
    print("Testing services...")
    
    try:
        # Test database connection
        from app.models import db
        db.session.execute('SELECT 1')
        print("âœ“ Database connection successful")
        
        # Test Redis connection (via Celery)
        from app.tasks.cv_tasks import health_check_task
        result = health_check_task.delay()
        health_status = result.get(timeout=10)
        if health_status['status'] == 'healthy':
            print("âœ“ Celery/Redis connection successful")
        else:
            print("âœ— Celery/Redis connection failed")
        
        # Test Gemini API
        gemini_key = os.environ.get('GEMINI_API_KEY')
        if gemini_key:
            print("âœ“ Gemini API key configured")
        else:
            print("âš  Gemini API key not configured")
        
        # Test Stripe API
        stripe_key = os.environ.get('STRIPE_SECRET_KEY')
        if stripe_key:
            print("âœ“ Stripe API key configured")
        else:
            print("âš  Stripe API key not configured")
        
        print("Service testing completed.")
        
    except Exception as e:
        print(f"âœ— Service test failed: {str(e)}")

if __name__ == '__main__':
    # Development server
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    print(f"Starting MorphCV Flask application on port {port}")
    print(f"Debug mode: {debug}")
    print(f"Environment: {os.environ.get('FLASK_ENV', 'development')}")
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=debug,
        threaded=True
    )

